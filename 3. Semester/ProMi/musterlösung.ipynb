{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProMI Exercise Sheet 1: Basic Probabilities & Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first exercise sheet is about the basics of probabilities and information theory.\n",
    "The exercises are a mix of theoretical questions and practical coding exercises.\n",
    "\n",
    "For the coding exercises, we will use Python as our main programming language and rely on the NumPy and Matplotlib packages.\n",
    "If you are not familiar with Python, Jupyter Notebooks, NumPy, or Matplotlib, we recommend you go through the following tutorial before starting with the exercises: https://cs231n.github.io/python-numpy-tutorial/.\n",
    "\n",
    "The theoretical questions can be answered by adding your answers in the markdown cells and using LaTeX to write down the mathematical formulas.\n",
    "Wrap your LaTeX formulas with a dollar sign, like this: `$ f(x) = 2 $` or `$$ f(x) = 2 $$` for a formula on a new line.\n",
    "These would show as $ f(x) = 2 $ and $$ f(x) = 2 $$\n",
    "You can double-click on a markdown cell to edit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_true(test_name, value: bool):\n",
    "    if value:\n",
    "        #print(\"\\033[92mTest {}: passed.\\033[0m\".format(test_name))\n",
    "        print(f'Test {test_name}: passed.')\n",
    "    else:\n",
    "        #print(\"\\033[91mTest {}: failed.\\033[0m\".format(test_name))\n",
    "        print(f'Test {test_name}: failed.')\n",
    "\n",
    "# equality\n",
    "def test_almost_equal(test_name, value: float, target: float, precision: float = 1e-4):\n",
    "    test_true(test_name, abs(value - target) < precision)\n",
    "\n",
    "# almost equal\n",
    "def test_almost_zero(test_name, value: float, precision: float = 1e-4):\n",
    "    test_almost_equal(test_name, value, 0.0, precision=precision)\n",
    "\n",
    "def test_almost_equal_array(test_name, value: np.ndarray, target: np.ndarray, precision: float = 1e-4):\n",
    "    test_true(test_name, np.all(np.abs(value - target) < precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basic Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1: Bonferroni Inequality\n",
    "\n",
    "Let $A$ and $B$ be two events in a probability space. Prove the following inequality (known as the Bonferroni inequality):\n",
    "$$ P(A, B) \\geq P(A) + P(B) - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "$$P(A) + P(B) - P(A, B) = P(A \\cup B)$$\n",
    "$$P(A \\cup B) \\leq 1$$\n",
    "$$P(A) + P(B) - P(A, B) \\leq 1$$\n",
    "$$P(A, B) \\geq P(A) + P(B) - 1$$\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: Simple Probability Density Functions\n",
    "\n",
    "The PDF of a random variable $X$ is given by:\n",
    "$$f(x) = \n",
    "\\begin{cases} \n",
    "c(2 - 2x^2) & \\text{for } -1 < x < 1 \\\\ \n",
    "0 & \\text{otherwise} \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find the value of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "Since $f(x)$ is a probability density function, it must integrate to 1 over its entire range:\n",
    "$$\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1.$$  \n",
    "Given $f(x) = 0$ for $ x \\leq -1 $ and $x \\geq 1$, we only need to integrate from $-1$ to $1$:\n",
    "$$\\int_{-1}^{1} c(2 - 2x^2) \\, dx = 1.$$\n",
    "Factor out $c$:\n",
    "$$c \\int_{-1}^{1} (2 - 2x^2) \\, dx = 1.$$\n",
    "Split integrals:\n",
    "$$c \\left( \\int_{-1}^{1} 2 \\, dx - \\int_{-1}^{1} 2x^2 \\, dx \\right) = 1.$$\n",
    "Integrate the first term:\n",
    "$$\\int_{-1}^{1} 2 \\, dx = 2x \\Big|_{-1}^{1} = 2 - (-2) = 4.$$\n",
    "Integrate the second term:\n",
    "$$\\int_{-1}^{1} 2x^2 \\, dx = \\frac{2}{3}x^3 \\Big|_{-1}^{1} = \\frac{2}{3} - \\left( -\\frac{2}{3} \\right) = \\frac{4}{3}.$$\n",
    "Substitute and solve for $c$:\n",
    "$$c \\left( 4 - \\frac{4}{3} \\right) = 1.$$\n",
    "$$c = \\frac{3}{8}.$$\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute the expected value $E[X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "$E[X]$ of a continuous random variable is given by:\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x f(x) \\, dx.$$\n",
    "Again, since $f(x) = 0$ for $x \\leq -1$ and $x \\geq 1$, we only need to integrate from $-1$ to $1$:\n",
    "$$E[X] = \\int_{-1}^{1} x \\cdot \\frac{3}{8} (2 - 2x^2) \\, dx.$$\n",
    "$$E[X] = \\frac{3}{8} \\int_{-1}^{1} 2x - 2x^3 \\, dx.$$\n",
    "Integrate the first term:\n",
    "$$\\int_{-1}^{1} 2x \\, dx = x^2 \\Big|_{-1}^{1} = 1 - (-1)^2 = 0.$$\n",
    "Integrate the second term:\n",
    "$$\\int_{-1}^{1} 2x^3 \\, dx = \\frac{1}{2}x^4 \\Big|_{-1}^{1} = \\frac{1}{2} - \\frac{1}{2} = 0.$$\n",
    "Therefore, $E[X] = 0$.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the CDF of $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "The CDF is defined as:\n",
    "$$F(x) = \\int_{-\\infty}^{x} f(x') \\, dx'.$$\n",
    "Since $f(x) = 0$ for $x \\leq -1$ and $x \\geq 1$, we consider three cases:\n",
    "1. For $x \\leq -1$, $F(x) = \\int_{-\\infty}^{x} 0 \\, dx' = 0$.\n",
    "2. For $-1 < x < 1$, $F(x) = \\int_{-1}^{x} \\frac{3}{8} (2 - 2x'^2) \\, dx'$.\n",
    "3. For $x \\geq 1$, $F(x) = 1$, since all the probability mass is contained within the range $-1 < x < 1$.\n",
    "\n",
    "Case 2:\n",
    "$$F(x) = \\frac{3}{8} \\int_{-1}^{x} 2 - 2x'^2 \\, dx'.$$\n",
    "$$F(x) = \\frac{3}{8} \\left( 2x' - \\frac{2}{3}x'^3 \\right) \\Big|_{-1}^{x}.$$\n",
    "$$F(x) = \\frac{3}{8} \\left( 2x - \\frac{2}{3}x^3 - (-2 + \\frac{2}{3}) \\right).$$\n",
    "$$F(x) = \\frac{3}{8} \\left( 2x - \\frac{2}{3}x^3 + \\frac{4}{3} \\right).$$\n",
    "$$F(x) = -\\frac{1}{4}x^3 + \\frac{3}{4}x + \\frac{1}{2}.$$\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Coding) Plot the PDF and CDF of $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return 3/8 * (2 - 2*x**2) if -1 < x < 1 else 0\n",
    "    ### END SOLUTION\n",
    "\n",
    "def cdf(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return -1/4 * x**3 + 3/4 * x + 1/2 if -1 < x < 1 else 0 if x <= -1 else 1\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-1.5, 1.5, 1000)\n",
    "\n",
    "pdf_values = [pdf(x) for x in xs]\n",
    "cdf_values = [cdf(x) for x in xs]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(xs, pdf_values, label='PDF')\n",
    "plt.title('Probability Density Function (PDF)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(xs, cdf_values, label='CDF')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('F(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3: De Morgan's Law\n",
    "Let $E_1$ and $E_2$ are mutually independent events. Show that $\\bar{E}_1$ and $\\bar{E}_2$ are also mutually indepent. Remark De Morgan's law: $p(\\bar{A},\\bar{B}) = p(\\overline{A \\cup B})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "We must show:\n",
    "$p(\\bar{E_1},\\bar{E_2})=p(\\bar{E_1})p(\\bar{E_2}) \\,.$\n",
    "\n",
    "De Morgan's law\n",
    "\n",
    "$$p(\\bar{E_1},\\bar{E_2})=p(\\overline{E_1 \\cup E_2})$$\n",
    "\n",
    "Further adjust terms:\n",
    "\n",
    "$$ p(\\overline{E_1 \\cup E_2}) = 1-p(E_1 \\cup E_2) = 1-(p(E_1)+p(E_2)-p(E_1,E_2)) = 1-p(E_1)-p(E_2)+p(E_1,E_2) $$\n",
    "\n",
    "Using the independence of $E_1$ and $E_2$ we get:\n",
    "$$1-p(E_1)-p(E_2)+p(E_1, E_2) = 1-p(E_1)-p(E_2)+p(E_1)p(E_2)$$\n",
    "\n",
    "We can factorize this as:\n",
    "\n",
    "$$1-p(E_1)-p(E_2)+p(E_1)p(E_2) = (1-p(E_1))(1-p(E_2)) = p(\\bar{E_1})p(\\bar{E_2})$$\n",
    "\n",
    "q.e.d.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4: LRU Technique \n",
    "\n",
    "Consider a system that cache files using a Least Recently Used (LRU) technique. In this system, the probability of requesting a popular file is $p(P) = 0.10$. If a cache's update happens, the probability of requesting a popular file is $p(P|U) = 0.20$. However, if no update has happened, the probability is only $p(P | \\bar{U}) = 0.05$. Find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The probability of an update $p(U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "Using the law of total probability:  \n",
    "$$p(P) = p(P | U)  p(U) + p(P | \\bar{U}) p(\\bar{U})$$\n",
    "\n",
    "Since $ p(\\bar{U}) = 1 - p(U)$, and substituing the given values:  \n",
    "$$0.10 = (0.20)  p(U) + (0.05)  (1 - p(U))$$  \n",
    "$$0.05 = 0.15  p(U)$$  \n",
    "$$p(U) = \\frac{1}{3} \\approx 0.333$$  \n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The probability of an update given a popular file was requested $p(U|P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "Using Bayes' rule and $p(U)$ from item **a**:  \n",
    "$$p(U | P) = \\frac{p(P | U) p(U)}{p(P)} = \\frac{0.20  \\frac{1}{3}}{0.10} = \\frac{2}{3} \\approx 0.667$$\n",
    "\n",
    "Thus, the probability of an update given a popular file was requested is 0.667.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5: Independence\n",
    "\n",
    "Due to variations in the distance from Earth, the arrival time of daily updates of a robot on Mars follows a uniform distribution between 7 AM and 8 AM. Based on the events $A = \\text{update has not arrived by 7:30 AM}$ and $B = \\text{update has arrived by 7:31 AM}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Are A and B independent? Prove your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "First, let's define $p(A)$ and $p(B)$:  \n",
    "$$p(A) = \\frac{\\text{length of interval between 7 AM and 7:30 AM}}{\\text{length of interval between 7 AM and 8 AM}} = \\frac{30}{60} = \\frac{1}{2}$$  \n",
    "$$p(B) = \\frac{\\text{length of interval between 7 AM and 7:31 AM}}{\\text{length of interval between 7 AM and 8 AM}} = \\frac{31}{60}$$\n",
    "$$p(A)p(B) =  \\frac{31}{120}$$\n",
    "\n",
    "Given the uniform distribution, $p(A,B)$, i.e., the probability of arriving between 7:30 AM and 7:31 AM, can also be defined:\n",
    "$$p(A,B) = \\frac{\\text{length of interval between 7:30 AM and 7:31 AM}}{\\text{length of interval between 7 AM and 8 AM}} = \\frac{1}{60}$$\n",
    "\n",
    "Therefore, the events are not independent since $p(A) p(B) \\neq p(A,B)$.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find:\n",
    "$p(B|A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "Using the conditional probability:  \n",
    "$$p(B|A) = \\frac{p(A,B)}{p(A)} = \\frac{\\frac{1}{60}}{\\frac{1}{2}} = \\frac{1}{30}$$\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find:\n",
    "$p(A|B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "Using the conditional probability again:  \n",
    "$$p(A|B) = \\frac{p(A,B)}{p(B)} = \\frac{\\frac{1}{60}}{\\frac{31}{60}} = \\frac{1}{31}$$\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.6: Probability Density Function\n",
    "\n",
    "Consider the random variable $X$ with pdf $f(x)$ given by:\n",
    "$$f(x) = \n",
    "\\begin{cases}\n",
    "a(1+x), & -1 < x \\leq 0 \\\\\n",
    "a(1-x), & 0 < x \\leq 1 \\\\\n",
    "0, & \\text{elsewhere}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Find the constant $a$ and plot $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "The total probability over the sample space is exactly one, therefore:\n",
    "$$\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1$$\n",
    "\n",
    "Substituting the definition of $f(x)$:\n",
    "$$\\int_{-1}^{0} a(1 + x) \\, dx + \\int_{0}^{1} a(1 - x) \\, dx = 1$$\n",
    "\n",
    "And solving both integrals:\n",
    "$$a \\left[ x + \\frac{x^2}{2} \\right]_{-1}^{0} + \\left[ x - \\frac{x^2}{2} \\right]_{0}^{1} = 1$$\n",
    "$$a \\left( 0 - \\left( -1 + \\frac{1}{2} \\right) \\right) + a\\left(\\left( 1 - \\frac{1}{2} \\right) - 0\\right) = 1 $$\n",
    "$$ \\frac{a}{2} + \\frac{a}{2} = 1 $$\n",
    "\n",
    "Thus, $a = 1$.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = 1\n",
    "def f(x):\n",
    "    if -1 < x <= 0:\n",
    "        return a*(1 + x)\n",
    "    elif 0 < x <= 1:\n",
    "        return a*(1 - x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "num_points = 1000\n",
    "eval_interval = [-1.5, 1.5]\n",
    "x_eval = np.linspace(eval_interval[0], eval_interval[1], num_points)\n",
    "f_vec = np.vectorize(f, otypes=[np.float32])  \n",
    "f_eval = f_vec(x_eval)\n",
    "\n",
    "# Plot f(x)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_eval, f_eval, label=r'$f(x)$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x)$')\n",
    "plt.title(r'pdf $f(x)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define the CDF $F(x)$ and plot it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "The CDF is defined as:\n",
    "$$F(x) = p(X \\leq x) = \\int_{-\\infty}^{x'} f(x') \\, dx'$$\n",
    "\n",
    "Evaluating for each interval:   \n",
    "For $x \\leq -1:$   \n",
    "$$F(x) = 0, \\text{since the pdf is zero for } x < -1 $$\n",
    "\n",
    "For $-1 < x \\leq 0:$   \n",
    "$$F(x) =  \\int_{-1}^{x} 1 + x' \\, dx' =  \\left[ x' + \\frac{x'^2}{2} \\right]_{-1}^{x} =  x + \\frac{x^2}{2} + \\frac{1}{2} = \\frac{1}{2}(1+x)^2$$\n",
    "\n",
    "For $0 < x \\leq 1:$   \n",
    "$$F(x) = F(0) + \\int_{0}^{x} 1 - x' \\, dx' =  \\frac{1}{2} + \\left[ x' - \\frac{x'^2}{2} \\right]_{0}^{x} = \\frac{1}{2} + x - \\frac{x^2}{2} = 1 - \\frac{1}{2}(1-x)^2$$\n",
    "\n",
    "\n",
    "For $x > 1:$   \n",
    "$$F(x) = 1, \\text{as the pdf is completly accumulated when } x = 1 $$\n",
    "\n",
    "Therefore,\n",
    "$$ F(x) = \n",
    "\\begin{cases}\n",
    "0, & x < -1 \\\\\n",
    "\\frac{1}{2}(1 + x)^2, & -1 < x \\leq 0 \\\\\n",
    "1 - \\frac{1}{2}(1 - x)^2, & 0 < x \\leq 1 \\\\\n",
    "1, & x > 1\n",
    "\\end{cases}$$\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def CDF(x):\n",
    "    if x < -1:\n",
    "        return 0\n",
    "    if -1 < x <= 0:\n",
    "        return 0.5*(1 + x)**2\n",
    "    elif 0 < x <= 1:\n",
    "        return 1.0 - 0.5*(1 - x)**2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "CDF_vec = np.vectorize(CDF, otypes=[np.float32])  \n",
    "CDF_eval = CDF_vec(x_eval)\n",
    "\n",
    "# Plot CDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_eval, CDF_eval, label=r'$F(x)$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$F(x)$')\n",
    "plt.title(r'CDF F(x)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Find point $b$ such that:\n",
    "$p(X > b) = \\frac{1}{2}p(X \\leq b)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "The events are complementary, therefore:\n",
    "$$p(X > b) = 1 - p(X \\leq b)$$\n",
    "\n",
    "Substituing the given condition $p(X > b) = \\frac{1}{2}p(X \\leq b)$:\n",
    "$$ \\frac{1}{2}p(X \\leq b) = 1 - p(X \\leq b)$$\n",
    "$$ p(X \\leq b) = \\frac{2}{3}$$\n",
    "\n",
    "Thus, $b \\in [-1,1]$, since the $p(x) = 0 $ for any $x \\notin [-1, 1]$. $p(X \\leq b)$ is computed by:\n",
    "$$p(X \\leq b) = \\int_{-1}^{b} f(x) \\, dx= F(b)$$\n",
    "\n",
    "From the CDF obtained in the previous item, we need to solve $b$ for each interval.   \n",
    "For $-1 < x \\leq 0:$   \n",
    "$$\\frac{1}{2} (1 + b)^2 = \\frac{2}{3}$$\n",
    "$$(1 + b) = \\pm \\frac{2}{\\sqrt{3}}$$\n",
    "$$b = \\pm \\frac{2}{\\sqrt{3}} - 1$$\n",
    "$$\\text{Since $b \\in [-1,0]$, no solution possible}$$\n",
    "\n",
    "For $0 < x \\leq  1:$   \n",
    "$$1 - \\frac{1}{2} (1 - b)^2 = \\frac{2}{3}$$\n",
    "$$(1 - b)^2 = \\frac{2}{3}$$\n",
    "$$b = 1 \\pm \\frac{\\sqrt{2}}{\\sqrt{3}}$$\n",
    "$$\\text{Since $b \\in [0,1]$}, b = 1 - \\frac{\\sqrt{2}}{\\sqrt{3}} \\approx 0.183 $$\n",
    "\n",
    "Therefore, $b \\approx 0.183$.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Joint Probability\n",
    "About joint probabilities, prove $\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} p(x,y) \\, dxdy = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "From the marginal probability:\n",
    "$$ p(y) = \\int_{-\\infty}^{\\infty} p(x,y) \\, dx$$\n",
    "Substituting this into the original integral:\n",
    "$$ \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} p(x,y) \\, dxdy = \\int_{-\\infty}^{\\infty}  p(y) \\, dy $$\n",
    "The integral equals 1 based on Probability Axiom 2.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Distribution Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1: Expectation & Variance\n",
    "\n",
    "We consider the coffee consumption of two Computer Science students, Alice and Bob.\n",
    "Let $X$ and $Y$ be random variables characterizing the number of coffees that Alice and Bob respectively drink in a day.\n",
    "These are **discrete** random variables.\n",
    "The possible numbers of daily coffees for each programmer are denoted as $\\mathcal{X}$ for Alice and $\\mathcal{Y}$ for Bob.\n",
    "We denote their probability mass functions as $p_X$ and $p_Y$.\n",
    "\n",
    "We model Alice's consumption as follows:\n",
    "- $\\mathcal{X} = \\{0, 1, 2, 3, 4, 5\\}$\n",
    "- $p_X = [0.05, 0.10, 0.40, 0.25, 0.10, 0.10]$\n",
    "\n",
    "where $p_{X}[x] = p_{X}(x), \\quad x \\in \\mathcal{X}$.\n",
    "\n",
    "We model Bob's consumption as follows:\n",
    "- $\\mathcal{Y} = \\{0, 1, 2, 3, 4, 5, 6, 7\\}$\n",
    "- $p_Y = [0.00, 0.01, 0.09, 0.20, 0.30, 0.20, 0.10, 0.10]$\n",
    "\n",
    "where $p_{Y}[y] = p_{Y}(y), \\quad y \\in \\mathcal{Y}$.\n",
    "\n",
    "For instance, the probability that Bob drinks 2 coffees today is $p_{Y}(2) = 0.09$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1a) Theoretical Questions\n",
    "#### 1. Give the definition of the expected value of $X$, denoted as $\\mathbb{E}[X]$.\n",
    "#### 2. Show that the expectation is linear, e.g. by showing that from $\\mathbb{E}[aX + bY] = a \\mathbb{E}[X] + b \\mathbb{E}[Y]$ for constants $a$ and $b$.\n",
    "#### 3. Give the definition of the variance of $X$, denoted as $\\mathbb{V}[X]$.\n",
    "#### 4. Show that $\\mathbb{V}[cX] = c^2 \\mathbb{V}[X]$ for any constant $c \\in \\mathbb{R}$.\n",
    "#### 5. Give the formula connecting the variance $\\mathbb{V}[X]$ and the standard deviation $\\sigma_X$ of $X$.\n",
    "#### 6. Considering $\\mathcal{X}$ and $p_X$:\n",
    "    1. Compute the expected value of $X$.\n",
    "    2. Compute the variance of $X$.\n",
    "    3. Compute the standard deviation of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "### BEGIN SOLUTION\n",
    "\n",
    "1. $$\\mathbb{E}[X] = \\sum_{x \\in \\mathcal{X}} x p(x)$$\n",
    "2. \n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbb{E}[aX + bY] & = \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} (a x + b y) p(x, y) \\\\\n",
    "& = \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} a x p(x, y) + \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} b y p(x, y) \\\\\n",
    "& = \\sum_{x \\in \\mathcal{X}} a x p(x) + \\sum_{y \\in \\mathcal{Y}} b y p(y) \\\\\n",
    "& = a \\sum_{x \\in \\mathcal{X}} x p(x) + b \\sum_{y \\in \\mathcal{Y}} y p(y) \\\\\n",
    "& = a\\mathbb{E}[X] + b \\mathbb{E}[Y]\n",
    "\\end{split}\n",
    "$$\n",
    "3. $$\\mathbb{V}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$\n",
    "\n",
    "4.\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbb{V}[cX] & = \\mathbb{E}[(cX)^2] - \\mathbb{E}[cX]^2 \\\\\n",
    "& = \\mathbb{E}[c^2 X^2] - (c \\mathbb{E}[X])^2 \\\\\n",
    "& = c^2 \\mathbb{E}[X^2] - c^2 \\mathbb{E}[X]^2 \\\\\n",
    "& = c^2 (\\mathbb{E}[X^2] - \\mathbb{E}[X]^2) \\\\\n",
    "& = c^2 \\mathbb{V}[X]\\end{split}\n",
    "$$\n",
    "5. $$\\sigma_{X}^{2} = \\mathbb{V}[X]$$\n",
    "6. 1:  E(X) = 2.55  \n",
    "    2: V(X) = 1.55  \n",
    "    3: $\\sigma_x  \\approx 1.24 $\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1b) Practical Questions\n",
    "We instantiate the set of possible values for $X$ and $Y$, and their associated possibilities below using numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.array([0, 1, 2, 3, 4, 5])\n",
    "x_probabilities = np.array([0.05, 0.10, 0.40, 0.25, 0.10, 0.10]) # NOTE: x_probabilities[i] == probability of x_values[i]\n",
    "y_values = np.array([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "y_probabilities = np.array([0.00, 0.01, 0.09, 0.20, 0.30, 0.20, 0.10, 0.10])\n",
    "\n",
    "# Sanity check, the probabilities must sum to 1 (taking into account numerical precision)\n",
    "assert np.isclose(x_probabilities.sum(), 1), \"Probabilities should sum to 1.\"\n",
    "assert np.isclose(y_probabilities.sum(), 1), \"Probabilities should sum to 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize their PMF below.\n",
    "Based on this visualization, think about which random variable has the highest variance and think about what should roughly be the expected value of each random variable. Check if it is coherent with what you computed for $X$.\n",
    "(This is not a question, just a first sanity check.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating figure with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(12, 4), squeeze=True)\n",
    "\n",
    "# x\n",
    "axs[0].bar(x_values, x_probabilities)\n",
    "axs[0].set_ylabel(\"p_X(x)\")\n",
    "axs[0].set_xlabel(\"x\")\n",
    "\n",
    "# y\n",
    "axs[1].bar(y_values, y_probabilities)\n",
    "axs[1].set_ylabel(\"p_Y(y)\")\n",
    "axs[1].set_xlabel(\"y\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement the functions `expectation`, `variance` and `standard_deviation` below to respectively compute the expected value, the variance and the standard deviation using the formula you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    ### BEGIN SOLUTION\n",
    "    return possible_values @ probabilities\n",
    "    ### END SOLUTION\n",
    "\n",
    "def variance(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    ### BEGIN SOLUTION\n",
    "    expected_value = expectation(possible_values, probabilities)\n",
    "    return expectation(possible_values ** 2, probabilities) - expected_value ** 2\n",
    "    ### END SOLUTION\n",
    "\n",
    "def standard_deviation(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    ### BEGIN SOLUTION\n",
    "    var = variance(possible_values, probabilities)\n",
    "    return np.sqrt(var)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.   Verify the implementation of the above functions:\n",
    "   \n",
    "   a. Does the value of $\\mathbb{E}[X]$ you computed previously match the output of your implemented function?\n",
    "   \n",
    "   b. Do you find that $\\mathbb{E}[Y]$ is roughly what you could have guessed by visualizing the PMF of $Y$?\n",
    "   \n",
    "   c. Do you find that $\\mathbb{V}[cX] = c^2 \\mathbb{V}[X]$ for any value of $c \\in \\mathbb{R}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# x\n",
    "mu_x = expectation(x_values, x_probabilities)\n",
    "var_x = variance(x_values, x_probabilities)\n",
    "sigma_x = standard_deviation(x_values, x_probabilities)\n",
    "\n",
    "# y\n",
    "mu_y = expectation(y_values, y_probabilities)\n",
    "var_y = variance(y_values, y_probabilities)\n",
    "sigma_y = standard_deviation(y_values, y_probabilities)\n",
    "\n",
    "# c x\n",
    "c = 1.5 \n",
    "cx_values = c * x_values\n",
    "mu_cx = expectation(cx_values, x_probabilities)\n",
    "var_cx = variance(cx_values, x_probabilities)\n",
    "sigma_cx = standard_deviation(cx_values, x_probabilities)\n",
    "\n",
    "print(f\"mu_x = {mu_x}, mu_y = {mu_y}, mu_cx = {mu_cx}\")\n",
    "print(f\"var_x = {var_x:.2f}, var_y = {var_y:.2f}, var_x = {var_cx:.2f}\")\n",
    "print(f\"sigma_x = {sigma_x:.2f}, sigma_y = {sigma_y:.2f}, sigma_cx = {sigma_cx:.2f}\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "2. 1. The value should be the same.\n",
    "   2. The guess based on PMF should make sense.\n",
    "   3. The equation should always work.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Interpret the results:\n",
    "   \n",
    "a. Who drinks the most coffee in a day in expectation?\n",
    "\n",
    "b. Who tends to have the most variable number of daily coffees?\n",
    "\n",
    "c. How many coffees in total do Alice and Bob drink daily in expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "3. 1. Bob drinks the most coffee.\n",
    "   2. Bob has a highest variability in the number of coffees he drinks daily.\n",
    "   3. They drink 6.84 coffees per day on average, using the linearity of expectation.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2: Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PhD student from IAS is attempting to log into a PC in the robotic lab.\n",
    "The problem is that this PhD student is used to using an AZERTY keyboard layout, while all PCs in the lab either use a QWERTY or QWERTZ keyboard layout on the login screen.\n",
    "We would like to study the number of trials this PhD student needs to successfully log in.\n",
    "We denote as $X$, $Y$, and $Z$ the random variables characterizing the number of attempts it takes for the student to successfully log in when using an AZERTY, a QWERTY, and a QWERTZ layout respectively.\n",
    "We model these distributions using independent geometric distribution.\n",
    "\n",
    "The geometric distribution models the occurrence of the first success in an endless series of independent and identically distributed Bernoulli trials.\n",
    "A Bernoulli trial, also known as a binomial trial, is a random experiment that results in one of two outcomes: \"success\" or \"failure,\" with the probability of success represented as $p_{success}$.\n",
    "The probability mass function for a geometric distribution is defined as follows:\n",
    "$$\n",
    "p(k) = (1 - p_{success})^{k-1} p_{success}\n",
    "$$\n",
    "and it denotes the probability of being successful on the $k$-th trial.\n",
    "\n",
    "In the described setting, we consider different probabilities of success for each keyboard layout:\n",
    "- AZERTY, where the success of an attempt is highly probable: $p_{succcess}^{X} = 0.9$,\n",
    "- QWERTY, where the success of an attempt is somewhat probable: $p_{succcess}^{Y} = 0.5$,\n",
    "- QWERTZ, where the success of an attempt is quite unlikely: $p_{succcess}^{Z} = 0.2$.\n",
    "\n",
    "We then note $p_X(k)$ as the probability that the PhD student successfully logs in on its exactly $k$-th attempt and use analogous notations for $p_Y$ and $p_Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2a) Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Give the definition of the kurtosis of a random variable $X$, denoted as $\\mathrm{Kurt}[X]$. You can use $\\mu$ and $\\sigma$ as symbols to denote the expected value and standard deviation of $X$ respectively.\n",
    "\n",
    "#### 2. Explain in one sentence what the kurtosis measures about a probabilistic distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "    \n",
    "1. $\\mathrm{Kurt[X] = \\mathbb{E} \\left[ \\left( \\frac{X - \\mu}{\\sigma} \\right)^4 \\right]} - 3$\n",
    "2. The kurtosis is a measure of \"how long\" the tail of a distribution is.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2b) Practical Questions\n",
    "\n",
    "We will rely on the `scipy` package to use the geometric distribution more easily.\n",
    "We start by instantiating a random variable for each keyboard layout and storing them in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probability of success at each trial for each keyboard layout\n",
    "ps_success = {\n",
    "    \"azerty\": 0.9,\n",
    "    \"qwerty\": 0.5,\n",
    "    \"qwertz\": 0.2,\n",
    "}\n",
    "\n",
    "# Random variables for each keyboard layout\n",
    "geoms = {layout: geom(p_success) for layout, p_success in ps_success.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the PMF for each keyboard layout. We plot in linear and logarithmic scale to see *how big* the tail of each distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pmf up to some maximum number of trials\n",
    "# We assume that higher values have too low probability to be\n",
    "# significant.\n",
    "max_num_trials = 15\n",
    "num_trials = np.arange(0, max_num_trials+1)\n",
    "pmfs = {layout: g.pmf(num_trials) for layout, g in geoms.items()}\n",
    "\n",
    "# Plot pmfs\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "for layout, g in geoms.items():\n",
    "    # linscale\n",
    "    axs[0].plot(num_trials, pmfs[layout], \"o\", label=layout)\n",
    "    axs[0].vlines(num_trials, 0, pmfs[layout])\n",
    "    # logscale\n",
    "    axs[1].plot(num_trials, pmfs[layout], \"o\", label=layout)\n",
    "    axs[1].set_yscale('log')\n",
    "axs[0].set_xlabel(\"Number of trials\")\n",
    "axs[1].set_xlabel(\"Number of trials\")\n",
    "axs[0].set_ylabel(\"Probability of success after exactly n trials\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that these distributions have different sizes of tail and it is where the kurtosis comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement the function `kurtosis` below to compute the kurtosis using the formula you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kurtosis(possible_values: np.ndarray, probabilities: np.ndarray):\n",
    "    ### BEGIN SOLUTION\n",
    "    mu = expectation(possible_values, probabilities)\n",
    "    sigma = standard_deviation(possible_values, probabilities)\n",
    "    return probabilities @ ((possible_values - mu) / sigma) ** 4 - 3\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compare the kurtosis values your function outputs against the kurtosis values computed by `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "kurtosis_scipy = {k: g.stats(\"k\") for k, g in geoms.items()}\n",
    "kurtosis_handcrafted = {k: kurtosis(num_trials, pmf) for k, pmf in pmfs.items()}\n",
    "\n",
    "print(f\"Kurtosis scipy = {kurtosis_scipy}\")\n",
    "print(f\"Kurtosis handcrafted = {kurtosis_handcrafted}\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Could you have anticipated that the distribution for certain keyboard layouts have higher/lower kurtosis? Explain.\n",
    "#### 4. In this particular context, give a semantic interpretation to the size of the tail of the distribution.\n",
    "#### 5. Do you notice how our way of computing the kurtosis underestimates the kurtosis of long-tailed distributions? Provide an explaination by thinking about possible wrong assumptions we have made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "3. It could have been anticipated from the PMF plot showing longer tails for QWERTZ.\n",
    "4. The bigger the size of the tail, the more trials the student typically requires to successfully log in.\n",
    "5. When we set max_num_trials to a low value, we truncate a big part of the long-tailed distribution and it is what it is not noticeable on small-tailed distributions but breaks with long-tailed once. \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.3: Median vs. Expectation\n",
    "\n",
    "Your favorite hard-drive manufacturer is happy to advertise that, in expectation, their hard drives break after only 100,000 hours of usage.\n",
    "Despite your trust in this company, you decide to investigate a little bit.\n",
    "\n",
    "You decide to assume that the time to failure/breakage follows an exponential distribution.\n",
    "Let $T$ be a **continuous** random variable following an exponential distribution, i.e. its probability density function (PDF) is defined as\n",
    "$$\n",
    "p_T(t) = \\lambda e^{-\\lambda t}, \\quad \\lambda > 0, \\quad t \\in [0, +\\infty).\n",
    "$$\n",
    "We consider $T$ to characterize the number of years it takes for a hard drive to break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3a) Theoretical Questions\n",
    "\n",
    "#### 1. Give a closed form for the expected value of $T$, denoted as $\\mathbb{E}[T]$.\n",
    "#### 2. Compute the value of $\\lambda$ that would yield an average lifetime of 100,000 hours, assuming $\\lambda$ is in breakages/second.\n",
    "#### 3. Give the cumulative density function (CDF) of $T$, denoted as $F_T(t)$, as a function of $p_t$.\n",
    "#### 4. Give closed-form for $F_T(t)$.\n",
    "#### 5. Give the definition of the median of a random variable.\n",
    "#### 6. Compute the median of $T$, denoted $\\mathrm{med}_T$ as a function of $\\lambda$, and give a numerical value using the value of $\\lambda$ you computed before.\n",
    "#### 7. How does the lifetime of 50\\% of the manufacturer's hard drive compare to the advertised expected lifetime?\n",
    "#### 8. Propose one or more possible issue(s) with our reasoning which might have led to unfair results to your favorite manufacturer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "1. $$\\mathbb{E}[T] = \\int_0^\\infty t\\lambda e^{-\\lambda t}dt = \\frac{1}{\\lambda}$$\n",
    "2. $$\\frac{1}{\\lambda} = 100 000 * 3600 \\Rightarrow \\lambda = 2.78^{-9} \\mathrm{ breakages/second}$$\n",
    "3. $$F_T(t) = \\int_{0}^{t} p_T(t') dt'$$\n",
    "NOTE: if using a lower bound of integral as $-\\infty$, it is still considered valid.\n",
    "\n",
    "4. $$F_T(t) = 1 - e^{-\\lambda t}$$\n",
    "\n",
    "5. The median $m$ is such that $F_T(m) = 0.5$, i.e. it splits the probability mass into two halves.\n",
    "\n",
    "6. $$\\mathrm{med}_T = \\frac{\\ln 2}{\\lambda} \\approx 69315 \\mathrm{hours}$$\n",
    "\n",
    "7. The median lifetime is more than 30% lower than the expected lifetime.\n",
    "\n",
    "8. We assumed that $T$ followed an exponential distribution, which might not be correct.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3b) Practical Questions\n",
    "Let us compute the median using Python.\n",
    "Firstly, we visualize the PMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 1e-5\n",
    "max_t = 1e6\n",
    "stepsize = 1000 # stepsize should be small for high lambdas\n",
    "assert lmbda > 0, \"The value of lambda should be strictly positive!\"\n",
    "\n",
    "# Discretizing set of ws, pdf and cdf\n",
    "t = np.linspace(0, max_t, num=int(max_t / stepsize))\n",
    "p_T = lmbda * np.exp(-lmbda * t)\n",
    "F_T = np.cumsum(p_T) * stepsize # emulating integration as sum of small rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "axs[0].plot(t, p_T)\n",
    "axs[0].set_xlabel(\"time to failure t [seconds]\")\n",
    "axs[0].set_ylabel(\"p_T(t)\")\n",
    "axs[1].plot(t, F_T)\n",
    "axs[1].set_ylabel(\"F_T(t)\")\n",
    "axs[1].set_xlabel(\"time to failure t [seconds]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement the function `get_theoretical_median` from the formula you derived before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_theoretical():\n",
    "    ### BEGIN SOLUTION\n",
    "    # Theoretical median\n",
    "    med_theoretical = np.log(2) / lmbda\n",
    "    return med_theoretical\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute the median by leveraging `F_T` that we computed before in the function `get_med_practical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_practical():\n",
    "    ### BEGIN SOLUTION\n",
    "    # Practical median\n",
    "    idx = (F_T > 0.5).nonzero()[0][0] # first index where F_w is greater than 0.5\n",
    "    med_practical = t[idx] # getting value of w associated to idx\n",
    "    return med_practical\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_theoretical = get_med_theoretical()\n",
    "med_practical = get_med_practical()\n",
    "print(f\"med_theo = {med_theoretical:.0f}, med_prac = {med_practical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. If there is a discrepency between the theoretical and the practical median, how do you explain it? How could you simply fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "    \n",
    "3. Setting a smaller stepsize would allow making numerical integration more accurate.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.4: Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4a) Theoretical Questions\n",
    "1. Give the definition of the covariance $\\mathrm{Cov}[X, Y]$ between two random variables $X$ and $Y$.\n",
    "2. Show that $\\mathbb{V}[X + Y] = \\mathbb{V}[X] + \\mathbb{V}[Y] + 2 \\mathrm{Cov}[X, Y]$. What does this formula become when $X$ and  $Y$ are independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "1. $\\mathrm{Cov}[X, Y] = \\mathbb{E} \\left[ \\left( X - \\mu_X \\right) \\left( Y - \\mu_Y \\right) \\right]$\n",
    "2. \n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbb{V}[X + Y] &= \\mathbb{E} \\left[ (X + Y)^2 \\right] - \\mathbb{E} \\left[ (X + Y) \\right]^2 \\\\\n",
    "&= \\mathbb{E} \\left[ X^2 + Y^2 + 2 X Y \\right] - \\left( \\mathbb{E} \\left[ X \\right] + \\mathbb{E} \\left[ Y \\right] \\right)^2 \\\\\n",
    "&= \\mathbb{E} \\left[ X^2 \\right] + \\mathbb{E} \\left[ Y^2 \\right] + 2 \\mathbb{E} \\left[ X Y \\right] - \\left( \\mathbb{E} \\left[ X \\right]^2 + \\mathbb{E} \\left[ Y \\right]^2 + 2 \\mathbb{E} \\left[ X \\right] \\mathbb{E} \\left[ Y \\right] \\right) \\\\\n",
    "&= \\mathbb{E} \\left[ X^2 \\right] + \\mathbb{E} \\left[ Y^2 \\right] + 2 \\mathbb{E} \\left[ X Y \\right] -  \\mathbb{E} \\left[ X \\right]^2 - \\mathbb{E} \\left[ Y \\right]^2 - 2 \\mathbb{E} \\left[ X \\right] \\mathbb{E} \\left[ Y \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ X^2 - \\mathbb{E} \\left[ X \\right]^2 \\right] + \\mathbb{E} \\left[ Y^2 - \\mathbb{E} \\left[ Y \\right]^2 \\right] + 2 \\left[ \\mathbb{E} \\left[X Y \\right] - \\mathbb{E} \\left[ X \\right] \\mathbb{E} \\left[Y \\right] \\right] \\\\\n",
    "&= \\mathbb{V}[X] + \\mathbb{V}[Y] + 2 \\mathrm{Cov}[X, Y]\n",
    "\\end{split}\n",
    "$$\n",
    "If $X$ and $Y$ are independent, then $\\mathrm{Cov}[X, Y] = 0$.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Entropy\n",
    "You are given an array of integers $X$ with $\\forall x \\in X: x \\geq 0$. Our goal is the calculate the entropy of $X$. Therefore follow the steps below:\n",
    "\n",
    "### 1.  Implement the function ```get_pmf_1d(X: np.array)``` which outputs the probability mass function for X, so p[x]=relative frequency of x in X. Think about for which indices $x$ p[x] must store a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmf_1d(X: np.array):\n",
    "    ### BEGIN SOLUTION\n",
    "    n = X.shape[0]\n",
    "    minimum, maximum = np.min(X), np.max(X)\n",
    "    assert minimum >= 0\n",
    "    pmf = np.zeros((maximum+1,))\n",
    "    for x in X:\n",
    "        pmf[x] += 1\n",
    "    return pmf / n\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),np.ones(10) * 0.1),(np.array([0, 2, 2, 5, 8, 1, 6, 0, 8, 9]),np.array([0.2, 0.1, 0.2, 0, 0, 0.1, 0.1, 0, 0.2, 0.1]))]\n",
    "            \n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = get_pmf_1d(X)\n",
    "    \n",
    "    test_almost_equal_array(f'{i}', pred, sol)\n",
    "    test_almost_equal(f'Sum {i}', np.sum(pred), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the function ```compute_entropy(p: np.array)``` which computes the entropy $H(X)$ of the probability mass function $p$ using get_mf_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(X: np.array):\n",
    "    ### BEGIN SOLUTION\n",
    "    probs = get_pmf_1d(X)\n",
    "    probs = probs[probs > 0]\n",
    "    return -np.sum(probs*np.log2(probs))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ((np.array([1, 2, 3, 4])), 2),\n",
    "    ((np.array([0, 0, 0, 0])), 0),\n",
    "]\n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = compute_entropy(X)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Compute for a Binary erasure channel the Channel Capacity (Theory question)\n",
    "In this question compute the channel capacity of a binary erasure channel with erasure probability $p_e = 0.25$ and put your result as latex code in the underlying markdown cell. Furthermore, what are the probabilities of the input $P(x)$ for which the channel capacity is achieved?\n",
    "\n",
    "The channel capacity is given by: \n",
    "\n",
    "$$C=\\max _{p(x)} \\mathrm{I}(X ; Y)$$\n",
    "\n",
    "*Hint:* Think about the probabilities of the input P(x). How are the input probabilities related to the max operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "The input probabilities that **maximize** the mutual information are P(X=0) = 0.5 and P(X=1) = 0.5. This can be shown by examining the formula for the mutual information:  \n",
    "$$\n",
    "I(X ; Y) = H(X) - H(X | Y).\n",
    "$$  \n",
    "\n",
    "\n",
    "#### Step 1: Entropy H(X)\n",
    "\n",
    "The entropy H(X) quantifies the average level of uncertainty about X. Entropy is **maximized** when uncertainty is **highest**, which occurs when all outcomes are equally likely.  \n",
    "\n",
    "In our case, X is a binary random variable, and the entropy is maximized when:  \n",
    "$$\n",
    "P(X=0) = P(X=1) = 0.5.\n",
    "$$  \n",
    "Thus, H(X) is maximized for a uniform input distribution.\n",
    "\n",
    "\n",
    "\n",
    "#### Step 2: Conditional Entropy H(X | Y)\n",
    "\n",
    "The conditional entropy H(X | Y) describes the uncertainty about X given the observation Y. A lower H(X | Y) indicates that Y provides more information about X.  \n",
    "\n",
    "The formula for the conditional entropy is:  \n",
    "$$\n",
    "H(X | Y) = \\sum_{y \\in \\mathcal{Y}} P(Y=y) H(X | Y=y),\n",
    "$$\n",
    "where we sum over the possible values of Y and their corresponding conditional entropies.  \n",
    "\n",
    "We now consider the three cases for the binary erasure channel (BEC):  \n",
    "\n",
    "1. **Case Y = 0 and Y = 1:**  \n",
    "   If Y = 0 or Y = 1, we know with certainty that X must be X = 0 or X = 1, respectively. This follows from the unique structure of Binary Erasure Channels. Since there is no uncertainty in these cases:  \n",
    "   $$\n",
    "   H(X | Y = 0) = 0 \\quad \\text{and} \\quad H(X | Y = 1) = 0.\n",
    "   $$  \n",
    "\n",
    "2. **Case Y = e (erasure):**  \n",
    "   When Y = e, the observation provides no information about X. Thus, the conditional entropy in this case is equal to the entropy of X:  \n",
    "   $$\n",
    "   H(X | Y = e) = H(X).\n",
    "   $$  \n",
    "\n",
    "\n",
    "#### Step 3: Substituting into the Conditional Entropy Formula\n",
    "\n",
    "Substituting these results into the formula for H(X | Y), we get:  \n",
    "$$\n",
    "H(X | Y) = P(Y = e) H(X | Y = e) + P(Y = 0) H(X | Y = 0) + P(Y = 1) H(X | Y = 1).\n",
    "$$  \n",
    "Simplifying:  \n",
    "$$\n",
    "H(X | Y) = P(Y = e) H(X) + P(Y = 0) ⋅ 0 + P(Y = 1) ⋅ 0.\n",
    "$$  \n",
    "Since P(Y = e) = 0.25 (the erasure probability), we have:  \n",
    "$$\n",
    "H(X | Y) = 0.25 H(X).\n",
    "$$  \n",
    "\n",
    "\n",
    "#### Step 4: Mutual Information\n",
    "\n",
    "Now, substituting H(X | Y) into the mutual information formula:  \n",
    "$$\n",
    "I(X ; Y) = H(X) - H(X | Y).\n",
    "$$  \n",
    "We get:  \n",
    "$$\n",
    "I(X ; Y) = H(X) - 0.25 H(X) = 0.75 H(X).\n",
    "$$  \n",
    "\n",
    "The mutual information I(X ; Y) is maximized when H(X) is maximized, which, as shown earlier, occurs when P(X=0) = P(X=1) = 0.5.\n",
    "\n",
    "\n",
    "\n",
    "#### Step 5: Calculating H(X) and Channel Capacity\n",
    "\n",
    "The entropy H(X) for a uniform input distribution is:  \n",
    "$$\n",
    "H(X) = -P(X=0) \\log_2 P(X=0) - P(X=1) \\log_2 P(X=1).\n",
    "$$  \n",
    "Substituting P(X=0) = P(X=1) = 0.5:  \n",
    "$$\n",
    "H(X) = -0.5 \\log_2 0.5 - 0.5 \\log_2 0.5 = 1 \n",
    "$$  \n",
    "\n",
    "Finally, the channel capacity C is:  \n",
    "$$\n",
    "C = \\max_{P(X)} I(X ; Y) = 0.75 ⋅ H(X).\n",
    "$$  \n",
    "Substituting H(X) = 1:  \n",
    "$$\n",
    "C = 0.75 ⋅ 1 = 0.75 \\, \\text{bits}.\n",
    "$$  \n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The channel capacity of the binary erasure channel is **0.75 bits**, achieved when the input probabilities are P(X=0) = P(X=1) = 0.5.\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Mutual Information\n",
    "Now you are given two arrays of integers $X$ and $Y$, where all elements are greater than or equal to 0 (cf. Question 1). Compute the mutual information $I(X;Y)$ between the two arrays.\n",
    "\n",
    "\n",
    "### 1. Implement the function ```get_pmf_2d(X: np.array, Y: np.array, max_y: int = None)``` which outputs the probability mass function for the joint probability of X and Y, so p[x, y]=relative frequency of x and y. \n",
    "\n",
    "Note: In this task ignore max_y. These function variable is necessary in a later task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmf_2d(X: np.array, Y: np.array, max_y: int = None):\n",
    "    ### BEGIN SOLUTION\n",
    "    n = X.shape[0]\n",
    "    assert Y.shape[0] == n\n",
    "    max_x = np.max(X)\n",
    "    if max_y is None:\n",
    "        max_y = np.max(Y)\n",
    "    pmf = np.zeros((max_x+1, max_y+1))\n",
    "    for x, y in zip(X, Y):\n",
    "        pmf[x, y] += 1\n",
    "    return pmf / n\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [((np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),np.eye(10,10) * 0.1),((np.array([0, 1, 2, 3]),np.array([3, 2, 2,1 ])),np.array([[0, 0, 0, 0.25], [0, 0, 0.25, 0], [0, 0, 0.25, 0], [0, 0.25, 0, 0]]))]\n",
    "            \n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = get_pmf_2d(*X)\n",
    "    test_almost_equal_array(f'{i}', pred, sol)\n",
    "    test_almost_equal(f'Sum {i}', np.sum(pred), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the function ```compute_mutual_information(X: np.array, Y: np.array)``` which computes the mutual information $I(X;Y)$ between the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_information(X: np.array, Y: np.array):\n",
    "    ### BEGIN SOLUTION\n",
    "    p_x = get_pmf_1d(X)\n",
    "    p_y = get_pmf_1d(Y)\n",
    "    p_xy = get_pmf_2d(X, Y)\n",
    "    m, n = np.max(X), np.max(Y)\n",
    "    mi = 0\n",
    "    for x in range(m+1):\n",
    "        if p_x[x] > 0:\n",
    "            for y in range(n+1):\n",
    "                if p_y[y] > 0 and p_xy[x, y] > 0:\n",
    "                    mi += p_xy[x, y] * np.log2(p_xy[x, y] / (p_x[x]*p_y[y]))\n",
    "    return mi\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ((np.array([1, 2, 3, 4]), np.array([0, 1, 2, 3])), 2),\n",
    "    ((np.array([1, 1, 2, 1]), np.array([1, 1, 1, 2])), 0.122556),\n",
    "]\n",
    "\n",
    "# X = np.random.choice(4, 10000, p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "\n",
    "for i, ((X, Y), sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = compute_mutual_information(X, Y)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Average Conditional Entropy of an Image\n",
    "\n",
    "Images can be represented in pixel matrices $M \\in \\mathcal{R}^{h\\times w \\times c}$, where $h$ is the height and $w$ is the width of the image, and $c$ is the channel dimension.\n",
    "In black-and-white images, $M[m,n]$ stores the pixel color of the $m$-th row and $n$-th column of the image, which is 1 for white and 0 for black.\n",
    "In RGB arrays you will instead have a 3D value for each pixel, with values between 0 and 255, but in this exercise, we focus on black-and-white images for simplicity.\n",
    "Storing such images can take up lots of storage, especially for high-resolution pictures.\n",
    "Instead, many image storage formats, such as JPEG, use a lossy compression of the images.\n",
    "The idea we consider here is that if we know the neighboring pixels of a pixel at position $(m,n)$, this information can allow us to predict what is the pixel value at $(m,n)$, e.g., using maximum likelihood.\n",
    "How good this approximation is can be estimated with the *average conditional entropy* of this relationship, which we will compute in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete the function `show_image` which is given a 2D pixel matrix of a black-and-white image as described before, and displays the image in a matplotlib plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img: np.array, title: str=''):\n",
    "    ### BEGIN SOLUTION\n",
    "    # Display the image\n",
    "    plt.imshow(img, cmap='gray', interpolation=None) #'nearest')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "height, width = 100, 100 \n",
    "bw_image = np.random.choice([0, 1], size=(height, width)) #, p=[0.01, 0.99])\n",
    "show_image(bw_image, title='Random BW Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. It will help us to define a 'code' for the neighbors of a pixel, which we do as follows:\n",
    "Looking at a pixel, we start with the upper left neighbor and go around the image clockwise to get the remaining neighbors (s. Example).\n",
    "The sequence of 0's and 1's that we encounter can be viewed as a binary number, which we can transform into an integer to obtain a decimal code of the neighbors.\n",
    "Compute the 'neighbor code' for a pixel in the function ```get_neighbor_code```.\n",
    "\n",
    "*Note:* Make sure that your function checks that the pixel index is not at the borders of the image (because not all neighbors are available), and if not returns the neighbor code -1.\n",
    "In practice, one could define a padding for such pixels, but we will only consider pixels that are properly inside the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(2, 2))\n",
    "numbers = np.array([\n",
    "    [1, 0, 1], \n",
    "    [0, -1, 0], \n",
    "    [1, 1, 1]\n",
    "])\n",
    "# Loop through each subplot and add a centered number from 1 to 9\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        number = numbers[i,  j]\n",
    "        axs[i, j].text(0.5, 0.5, str(number), ha='center', va='center', fontsize=20)\n",
    "        axs[i, j].axis('off')  # Turn off the axes for a cleaner look\n",
    "        axs[i, j].set_xticks([])  # Remove x-axis ticks\n",
    "        axs[i, j].set_yticks([])  # Remove y-axis ticks\n",
    "        axs[i, j].spines['top'].set_visible(True)      # Enable top border\n",
    "        axs[i, j].spines['bottom'].set_visible(True)   # Enable bottom border\n",
    "        axs[i, j].spines['left'].set_visible(True)     # Enable left border\n",
    "        axs[i, j].spines['right'].set_visible(True)    # Enable right border\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** In the plot above, the sequence is '10101110', which yields the neighbor code 174."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_code(pixels: np.array, m: int, n: int):\n",
    "    ### BEGIN SOLUTION\n",
    "    # m col, n row\n",
    "    w, h = pixels.shape\n",
    "    if m<1 or m>w-2 or n<1 or n>h-2:\n",
    "        return -1\n",
    "    neighbors = f'{pixels[m-1, n-1]}{pixels[m-1, n]}{pixels[m-1, n+1]}{pixels[m, n+1]}{pixels[m+1, n+1]}{pixels[m+1, n]}{pixels[m+1, n-1]}{pixels[m, n-1]}'\n",
    "    return int(neighbors, base=2)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ((np.array([\n",
    "        [1, 0, 1], \n",
    "        [0, -1, 0], \n",
    "        [1, 1, 1]\n",
    "    ]), 1, 1), 174),\n",
    "    ((np.array([\n",
    "        [1, 0, 1], \n",
    "        [0, -1, 0], \n",
    "        [1, 1, 1]\n",
    "    ]), 0, 1), -1),\n",
    "]\n",
    "\n",
    "# X = np.random.choice(4, 10000, p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "\n",
    "for i, ((X, m, n), sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = get_neighbor_code(X, m, n)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now implement two functions:\n",
    "\n",
    "i. ```compute_px```computes a 1D probability distribution where p[x]=probability of seeing x in the whole image array, with $x\\in\\{0,1\\}$. \n",
    "\n",
    "ii. ```compute_pxy```computes the joint probability of the pixels and their neighbors. Here we will use the neighbor code from 4b) to translate the neighbors of a pixel into a proper index for our array.  Therefore p[x,y] will hold the joint probability of seeing the middle pixel $x$ and the neighbor code $y$.\n",
    "\n",
    "*Note:* As described, we only consider pixels that are not at the border, so only consider these pixels for your computations.\n",
    "\n",
    "*Note:* Both distributions should have the proper shape. Think about what this shape must be for p[x,y] (think about the range of y values possible).\n",
    "\n",
    "*Note:* Think about how many different neighbor codes are possible. Adjust the get_pmf_2d function from above to use the extra information we have about the distribution Y. Hereby set the function variable max_y to the number of different neighbor codes.\n",
    "\n",
    "*Hint:* The functions ```get_pmf_1d``` and ```get_pmf_2d```may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_px(pixels: np.array):\n",
    "    ### BEGIN SOLUTION\n",
    "    pixels = pixels[1:pixels.shape[0]-1,1:pixels.shape[1]-1]\n",
    "    return get_pmf_1d(pixels.flatten())\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pxy(pixels: np.array):\n",
    "    ### BEGIN SOLUTION\n",
    "    w, h = pixels.shape\n",
    "    num_possible_codes = 2**8 \n",
    "    \n",
    "    X,Y = [] , []\n",
    "    for m in range(1, h-1):\n",
    "        for n in range(1, w-1):\n",
    "            X.append(pixels[m, n])\n",
    "            neighbor_code = get_neighbor_code(pixels, m, n)\n",
    "            assert neighbor_code >= 0 and neighbor_code < num_possible_codes\n",
    "            Y.append(neighbor_code)\n",
    "    \n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    return get_pmf_2d(X, Y,max_y=num_possible_codes)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = []\n",
    "i1 = np.array([\n",
    "        [1, 0, 1], \n",
    "        [0, 1, 0], \n",
    "        [1, 1, 1]\n",
    "])\n",
    "p_x1 = np.array([0,1])\n",
    "p_xy1 = np.zeros((2, 257))\n",
    "p_xy1[1, 174] = 1\n",
    "\n",
    "test_cases.append(\n",
    "    (i1, (p_x1, p_xy1))\n",
    ")\n",
    "\n",
    "\n",
    "for i, (X, (s1, s2)) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred1 = compute_px(X)\n",
    "    pred2 = compute_pxy(X)\n",
    "    test_almost_equal_array(f'{i+1}.1', pred1, s1)\n",
    "    test_almost_equal_array(f'{i+1}.2', pred2, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using the defined functions, now compute the average conditional entropy of the pixels and their neighbors. Implement your solution in the function ```compute_average_conditional_entropy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cond_entropy(pixels: np.array):\n",
    "    ### BEGIN SOLUTION\n",
    "    p_x = compute_px(pixels)\n",
    "    p_xy = compute_pxy(pixels)\n",
    "    result = 0\n",
    "    num_possible_codes = 2**8 \n",
    "    for x in range(2):\n",
    "        if p_x[x] > 0:\n",
    "            for y in range(num_possible_codes):\n",
    "                if p_xy[x, y] > 0:\n",
    "                    result -= p_xy[x, y] * np.log2(p_xy[x, y]/p_x[x])\n",
    "    return result\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "height, width = 100, 100  # Example size, can be adjusted\n",
    "# Generate a random black-and-white image (binary values 0 and 1)\n",
    "bw_image = np.random.choice([0, 1], size=(height, width))\n",
    "test_cases = [\n",
    "    (bw_image, 7.96487)\n",
    "]\n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = compute_cond_entropy(X)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Compute the KL divergence between these two distributions in both directions.\n",
    "\n",
    "In this task, we want to compute the KL divergence between two distributions. To compute the KL divergence we first need 2 distributions that we wanna compare. In our case, we take the following 2 distributions\n",
    "\n",
    "$\n",
    "X_1 \\sim \\mathcal{N}(0, 1)\n",
    "$\n",
    "\n",
    "$\n",
    "X_2 \\sim \\mathcal{N}(5, 6^2)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PDF of a normal distribution\n",
    "\n",
    "First, we want to be able to compute the probability of a value $x$ for the given 2 distributions. Therefore implement the function ```normal_pdf(x: float, mean: float, sigma: float)``` which computes the probability of values $x$ (array of values) for a normal distribution with mean $mean$ and standard deviation $sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x:np.ndarray, mu:float, sigma:float):\n",
    "    ### BEGIN SOLUTION\n",
    "    first_term = 1/(np.sqrt(2*np.pi*sigma**2))\n",
    "    second_term = np.exp(-0.5*((x-mu)**2/sigma**2))\n",
    "    return first_term*second_term\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [(np.array([0.0,-1.0,1.0,1.5,-1.5]), np.array([0.39894228, 0.24197072, 0.24197072, 0.1295176,  0.1295176])),(np.array([0.0,-1.0,1.0,1.5,-1.5]),[0.04400817, 0.04102012, 0.04648189, 0.04749133, 0.03937169])]\n",
    "\n",
    "mu = [0,4]\n",
    "sigma = [1,8]\n",
    "y = normal_pdf(np.array([0.0,-1.0,1.0,1.5,-1.5]), 4, 8)\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = normal_pdf(X,mu[i],sigma[i])\n",
    "    print(pred)\n",
    "    test_almost_equal_array(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function is implemented correctly, you will see a plot containting both normal distributions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.linspace(-30, 30, 10000)\n",
    "\n",
    "y1 = normal_pdf(x, 0, 1)\n",
    "y2 = normal_pdf(x, 5, 6)\n",
    "\n",
    "plt.plot(x, y1, label='mu=0, sigma=1')\n",
    "plt.plot(x, y2, label='mu=5, sigma=6')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the function ```kl_divergence(mean1: float, sigma1: float, mean2: float, sigma2: float)``` which computes the KL divergence between the two normal distributions with the given parameters.\n",
    "The general formula for the KL divergence is $D_{\\mathrm{KL}}[p: q]=\\int p \\log \\frac{p}{q} \\mathrm{~d} \\mu$ where $p$ and $q$ are the two distributions we want to compare.\n",
    "In our case, we have 2 normal distributions which has the advantage that a closed-form solution for the KL divergence exists. The KL divergence between two normal distributions is given by:\n",
    "$D_{\\text{KL}}(p \\,||\\, q) = \\log\\left(\\frac{\\sigma_q}{\\sigma_p}\\right) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2 \\sigma_q^2} - \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu1,sigma1,mu2,sigma2):\n",
    "    ### BEGIN SOLUTION\n",
    "    #    \n",
    "    return np.log(sigma2/sigma1) + (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2) - 0.5\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [((0,1,0,1), 0.0),((9,2,3,8),1.1987943611198906),((12,4,0,1),78.1137056388801)]\n",
    "\n",
    "\n",
    "for i, (X, sol) in enumerate(test_cases):\n",
    "    # Compute DGL step using your function\n",
    "    pred = kl_divergence(*X)\n",
    "    print(pred)\n",
    "    test_almost_equal(f'{i}', pred, sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now compute the KL divergence for the two given normal distributions.\n",
    "\n",
    "Hereby compute the KL-divergence once where x1 corresponds to p and x2 corresponds to q and once where x1 corresponds to q and x2 corresponds to p. Save the results in the variables kl1 and kl2 and print them. What do you observe and why? Answer this question in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl1 = kl_divergence(0,1,5,6)\n",
    "kl2 = kl_divergence(5,6,0,1)\n",
    "\n",
    "print(f\"KL-divergence between X1 and X2: {kl1}\")\n",
    "print(f\"KL-divergence between X2 and X1: {kl2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "The computed kl-divergecnes are different. This is the case as the kl-divergence is not symmetric. This is a reason why the kl-divergence is a divergence and not a distance measure.\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}