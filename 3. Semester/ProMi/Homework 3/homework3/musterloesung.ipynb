{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9699f2ad",
   "metadata": {},
   "source": [
    "# Promi Exercise Sheet 3: Estimation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e671ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37142144",
   "metadata": {},
   "source": [
    "# Part 1: Regression\n",
    "In this exercise, you will work on linear regression with polynomial features where we model the function $f(\\bm{x})$ as\n",
    "\n",
    "$$f(x) = \\bm{w}^\\intercal \\phi(x).$$\n",
    "\n",
    "The true model is a polynomial of degree 3\n",
    "\n",
    "$$f(x) = 0.5 + (2x - 0.5)^2 + x^3$$\n",
    "\n",
    "We further introduce noise into the system by adding a noise term $\\varepsilon_i$ which is sampled from a Gaussian distribution\n",
    "\n",
    "$$y = f(x) + \\varepsilon_i, \\quad\\varepsilon_i\\sim \\mathcal{N}(0, \\sigma^2).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ddd1e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  \"\"\"The true polynomial that generated the data D\n",
    "  Args:\n",
    "    x: Input data\n",
    "  Returns:\n",
    "    Polynomial evaluated at x\n",
    "  \"\"\"\n",
    "  return x ** 3 + (2 * x - .5) ** 2 + .5\n",
    "\n",
    "def generate_data(n, minval, maxval, variance=1., train=False, seed=0):\n",
    "  \"\"\"Generate the datasets. Note that we don't want to extrapolate,\n",
    "  and such, the eval data should always lie inside of the train data.\n",
    "  Args:\n",
    "    n: Number of datapoints to sample. n has to be atleast 2.\n",
    "    minval: Lower boundary for samples x\n",
    "    maxval: Upper boundary for samples x\n",
    "    variance: Variance or squared standard deviation of the model\n",
    "    train: Flag deciding whether we sample training or evaluation data\n",
    "    seed: Random seed\n",
    "  Returns:\n",
    "    Tuple of randomly generated data x and the according y\n",
    "  \"\"\"\n",
    "  # Set numpy random number generator\n",
    "  rng = np.random.default_rng(seed)\n",
    "\n",
    "  # Sample data along the x-axis\n",
    "  if train:\n",
    "    # We first sample uniformly on the x-Axis\n",
    "    x = rng.uniform(minval, maxval, size=(n - 2,))\n",
    "    # We will sample on datapoint beyond the min and max values to\n",
    "    # guarantee that we do not extrapolate during the evaluation\n",
    "    margin = (maxval - minval) / 100\n",
    "    min_x = rng.uniform(minval - margin, minval, (1,))\n",
    "    max_x = rng.uniform(maxval, maxval + margin, (1,))\n",
    "    x = np.concatenate((x, min_x, max_x))\n",
    "  else:\n",
    "    x = rng.uniform(minval, maxval, size=(n,))\n",
    "  eps = rng.standard_normal(n)\n",
    "\n",
    "  #\n",
    "  return x, f(x) + variance * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc1e17",
   "metadata": {},
   "source": [
    "## Question 1.1: Least Squares Linear Regression\n",
    "To carry out regression, we first need to define the basis functions $\\phi(\\mathbf{x})$. In this task we would like to use polynomial features of degree $k$. Furthermore, we implement ridge regression as we want to counteract overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2b1bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, degree):\n",
    "  \"\"\"\n",
    "  A polynomial feature function of degree n.\n",
    "  :param x: input of size (N,)\n",
    "  :param degree: polynomial degree\n",
    "  :return: Polynomial features evaluated at x of dim (degree, N)\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  features = np.zeros((degree + 1, x.shape[0]))\n",
    "  for i in range(degree + 1):\n",
    "    features[i] = x ** i\n",
    "  return features\n",
    "  ### END SOLUTION\n",
    "\n",
    "def fit_w(x, y, lam, degree):\n",
    "  \"\"\"\n",
    "  Fit the weights with the closed-form solution of ridge regression.\n",
    "  Args:\n",
    "    x: Input of size (N,)\n",
    "    y: Output of size (N,)\n",
    "    lam: Regularization parameter lambda\n",
    "    degree: Polynomial degree\n",
    "  Returns:\n",
    "    Optimal weights\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  features = polynomial_features(x, degree)\n",
    "  M = features.shape[0]\n",
    "  kernel = features @ features.T # np.einsum(\"ij,kj->ik\", features, features)\n",
    "  inverse = np.linalg.inv(kernel + lam * np.eye(M))\n",
    "  return inverse @ features @ y # np.einsum(\"ij,jk,k->i\", inverse, features, y)\n",
    "  ### END SOLUTION\n",
    "\n",
    "def predict(x, w, degree):\n",
    "  \"\"\"\n",
    "  Calculate the generalized linear regression estimate w^T phi(x) given x,\n",
    "  the feature function, and weights w.\n",
    "  Args:\n",
    "    x: input of size (N,)\n",
    "    w: Weights of size (M)\n",
    "    degree: Polynomial degree\n",
    "  Returns:\n",
    "    Generalized linear regression estimate\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  features = polynomial_features(x, degree)\n",
    "  return w.T @ features #np.einsum(\"i,ij->j\", w, features)\n",
    "  ### END SOLUTION\n",
    "\n",
    "def calc_mse(x, y):\n",
    "  \"\"\"\n",
    "  Calculates the mean squared error (MSE) between x and y\n",
    "  Args:\n",
    "    x: Data x of size (N,)\n",
    "    y: Data y of size (N,)\n",
    "  Returns:\n",
    "    MSE\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  return np.mean((x-y) ** 2)\n",
    "  ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbe561",
   "metadata": {},
   "source": [
    "Here you can try out your code by simply running the following cell. This cell will carry out your ridge regression implementation from above for $\\lambda=0$ in which case we are provided with the linear least squares solution.\n",
    "\n",
    "We evaluate the regression task on six different polynomial sizes $k = \\{0,1,3,5,7,9\\}$ based on your implementation of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Settings\n",
    "n_train = 15\n",
    "n_test = 100\n",
    "minval = -2.\n",
    "maxval = 2\n",
    "\n",
    "\n",
    "train_data = generate_data(n_train, minval, maxval, train=True, seed=1001)\n",
    "test_data = generate_data(n_test, minval, maxval, train=False, seed=1002)\n",
    "\n",
    "def plot_linear_regression(x, y, labels, eval_quantity):\n",
    "  \"\"\"Plotting functionality for the prediction of linear regression\n",
    "  for K different polynomial degrees.\n",
    "  Args:\n",
    "    x: Data of size (K, N). The first dimension denotes the different\n",
    "      polynomial degrees that has been experimented with\n",
    "    y: Data of size (K, N)\n",
    "  \"\"\"\n",
    "  K = x.shape[0]\n",
    "  colors = matplotlib.colormaps['Reds'].resampled(K+1)(range(1, K+1))\n",
    "  fig = plt.figure()\n",
    "  plt.scatter(test_data[0], test_data[1], color=\"tab:orange\", linewidths=0.5, label=\"Test\", alpha=0.3)\n",
    "  plt.scatter(train_data[0], train_data[1], color=\"tab:blue\", linewidths=0.5, label=\"Train\", alpha=0.3)\n",
    "  for i in range(K):\n",
    "    plt.plot(x[i], y[i], label=f\"{eval_quantity}={labels[i]}\", color=colors[i], lw=2.5)\n",
    "  plt.xlabel(\"x\")\n",
    "  plt.ylabel(\"y\")\n",
    "  plt.title('Data Points and Regression Curves')\n",
    "  plt.legend()\n",
    "\n",
    "def plot_mse(mse, labels):\n",
    "  \"\"\"Plotting functionality of the MSE for K different polynomial degrees.\"\"\"\n",
    "  fig = plt.figure()\n",
    "  plt.plot(labels, mse)\n",
    "  plt.scatter(labels, mse)\n",
    "  plt.xticks(labels)\n",
    "  plt.ylabel(\"MSE\")\n",
    "  plt.xlabel(\"Polynomial Degree\")\n",
    "  plt.title(\"Mean Squared Error for Different Polynomial Degrees\")\n",
    "\n",
    "# Evaluate regression for different polynomial degrees\n",
    "degrees = [0, 1, 3, 5, 7, 9]\n",
    "xs, ys, mse = [], [], []\n",
    "for degree in degrees:\n",
    "  w = fit_w(\n",
    "      train_data[0], train_data[1],\n",
    "      lam=0., # Edge case resulting in linear least squares regression\n",
    "      degree=degree\n",
    "  )\n",
    "  # Predict the test data\n",
    "  y_test = predict(test_data[0], w, degree)\n",
    "  mse.append(calc_mse(y_test, test_data[1]))\n",
    "\n",
    "  # Run regression over the whole interval\n",
    "  xs.append(np.linspace(minval, maxval, 100))\n",
    "  ys.append(predict(xs[-1], w, degree))\n",
    "xs = np.stack(xs)\n",
    "ys = np.stack(ys)\n",
    "\n",
    "plot_linear_regression(xs, ys, labels=degrees, eval_quantity=\"k\")\n",
    "plot_mse(mse, degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610befd",
   "metadata": {},
   "source": [
    "## Question 1.2: Interpretation of the results\n",
    "Please describe your results below in a few lines thereby answering which model you would choose and which phenomenon we see for small and large polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90044385",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "The mean squared error is minimized for the polynomial degrees of 3, 5, and 7. The observation fits well with the definition of the true function being of polynomial degree 3. For smaller polynomial degrees, the features are not expressive enough to predict the non-linearity of the data - underfitting. For larger polynomial degrees we can see overfitting.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee73e0",
   "metadata": {},
   "source": [
    "## Question 1.3: Bias Variance Tradeoff in Linear Regression\n",
    "\n",
    "The MSE can be used to quantify the performance of our estimator, $\\hat{f}$. The MSE can be decomposed into a form involving the bias and variance, giving rise to the bias-variance tradeoff. Suppose we would like to fit a model to regress on a query datapoint ($y(\\bm{x}_q), \\bm{x}_q$):\n",
    "\n",
    "$$\n",
    "y(\\bm{x}_q) = f(\\bm{x}_q) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}[\\epsilon] = 0$ and $Var[\\epsilon] = \\sigma_\\epsilon^2$. The loss is quantified as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) &=  \\mathbb{E}_{\\mathcal{D}, \\epsilon} \\left[ (y(\\bm{x}_q) - \\hat{f}_{\\mathcal{D}}(\\bm{x}_q))^2 \\right] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Show that this can be reduced to sum of the bias and variance, i.e.:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) = \\sigma^2_{\\epsilon} + \\text{bias}^2 + \\text{var}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b69ce",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "We begin by adding and subtracting the expected estimator $f = \\mathbb{E}[y(\\bm{x}_q)]$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) = \\mathbb{E}_{\\mathcal{D}, \\epsilon} \\left[ \\left( y(\\bm{x}_q) - f(\\bm{x}_q) + f(\\bm{x}_q) - \\hat{f}_{\\mathcal{D}}(\\bm{x}_q) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Next, we expand using the property $(a + b)^2 = a^2 + b^2 + 2ab$,  with\n",
    "- $a= y(\\bm{x}_q) - f(\\bm{x}_q)$\n",
    "- $b= f(\\bm{x}_q) - \\hat{f}_{\\mathcal{D}}(\\bm{x}_q)$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) = \\mathbb{E}_{\\mathcal{D}, \\epsilon} \\left[ \\Big( y(\\bm{x}_q) - f(\\bm{x}_q) \\Big)^2 + \\Big(f(\\bm{x}_q) - \\hat{f}_{\\mathcal{D}}(\\bm{x}_q) \\Big)^2 + 2\\Big( \\Big(y(\\bm{x}_q) - f(\\bm{x}_q)\\Big) \\Big(f(\\bm{x}_q) -  \\hat{f}_{\\mathcal{D}}(\\bm{x}_q) \\Big) \\Big) \\right]\n",
    "$$\n",
    "\n",
    "Because $\\mathbb{E}[\\epsilon] = 0$ and $\\epsilon = y(\\bm{x}_q) - f(\\bm{x}_q)$, the last term:\n",
    "\n",
    "$$\n",
    "2 \\epsilon \\Big(f(\\bm{x}_q) -  \\hat{f}_{\\mathcal{D}}(\\bm{x}_q) \\Big)\n",
    "$$\n",
    "\n",
    "vanishes, and the first term is the variance of $\\epsilon$, that is: $\\sigma_\\epsilon^2$. This leaves us with:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) = \\sigma_\\epsilon^2 + \\mathbb{E}_{\\mathcal{D}} \\left[\\Big(f(\\bm{x}_q) -  \\hat{f}_{\\mathcal{D}}(\\bm{x}_q) \\Big)^2 \\right]\n",
    "$$\n",
    "\n",
    "Using $\\bar{\\hat{f}}(\\bm{x}_q) = \\mathbb{E}_\\mathcal{D} \\left[ \\hat{f}_\\mathcal{D}(\\bm{x}_q) \\right]$, we obtain:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathcal{D}} \\left[\\left(f(\\bm{x}_q) -  \\hat{f}_{\\mathcal{D}}(\\bm{x}_q)\\right)^2 \\right] &= \\mathbb{E}_{\\mathcal{D}} \\left[ \\left( f(\\bm{x}_q) - \\bar{\\hat{f}}(\\bm{x}_q) + \\bar{\\hat{f}}(\\bm{x}_q) - \\hat{f}_{\\mathcal{D}}(\\bm{x}_q) \\right)^2 \\right] \\\\\n",
    "&= \\mathbb{E}_{\\mathcal{D}}\\left[f(\\bm{x}_q) -  \\bar{\\hat{f}}(\\bm{x}_q)\\right]^2 +  \\mathbb{E}_{\\mathcal{D}} \\left[\\left(\\bar{\\hat{f}}(\\bm{x}_q) -  \\hat{f}_{\\mathcal{D}}(\\bm{x}_q)\\right)^2 \\right] \\\\\n",
    "&= \\text{bias}^2 + \\text{var}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "by using the same argument as done above.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb750a",
   "metadata": {},
   "source": [
    "## Question 1.4: Empirical evaluation of the bias and the variance\n",
    "The bias-variance tradeoff is typically a purely theoretical concept as it requires the evaluation of $f(x)$ which is commonly not available. In this task we assume that $f(x)$ is known and thus, an approximation of the bias and variance is possible. Instead of the pointwise bias and variance based on a query $\\bm{x}_q$, we introduce the lecture bias and variance as an expected value over an evaluation dataset $\\mathcal{D}^\\prime$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{Bias}(\\hat{f}) &= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[\\text{bias}^2(\\hat{f}(\\bm{x}_q))] &&= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[f(\\bm{x}_q) - \\bar{\\hat{f}}(\\bm{x}_q)],\\\\\n",
    "    \\text{Var}(\\hat{f}) &= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[\\text{var}(\\hat{f}(\\bm{x}_q))] &&= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[\\mathbb{E}_{\\tilde{\\mathcal{D}} \\subset\\mathcal{D}}[(\\bar{\\hat{f}}(\\bm{x}_q) - \\hat{f}_{\\tilde{\\mathcal{D}}}(\\bm{x}_q))^2]].\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here, $\\bar{\\hat{f}}$ is the average prediction of the maximum likelihood estimator over various datasets sampled from the ground truth distribution\n",
    "$$\\bar{\\hat{f}}(\\bm{x}_q) = \\mathbb{E}_{\\tilde{\\mathcal{D}} \\subset\\mathcal{D}}[f_{\\tilde{\\mathcal{D}}}(\\bm{x}_q)].$$\n",
    "\n",
    "To empirically study the effect of the Bias and the Var, we approximate the expectations with Monte Carlo samples as follows\n",
    "$$\\bar{\\hat{f}}(x) \\approx \\frac{1}{M}\\sum_{j=1}^{M}\\left(\\hat{f}_{\\mathcal{D_j}}(x)\\right).$$\n",
    "\n",
    "$$\\text{Bias}[\\hat{f}_\\mathcal{D}] \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\left(f(x_i) - \\bar{\\hat{f}}(x_i)\\right)^2,$$\n",
    "\n",
    "$$\\text{Var}\\left[\\hat{f}_\\mathcal{D}\\right] \\approx \\frac{1}{NM}\\sum_{i=1}^{N}\\sum_{j=1}^{M}\\left(\\hat{f}_{\\mathcal{D}_j}(x_i) - \\bar{\\hat{f}}(x_i)\\right)^2$$\n",
    "\n",
    "Note that the indice $j=1,\\dots,M$ refer to subsets $\\tilde{\\mathcal{D}}_i$ from the dataset $\\mathcal{D}$, whereas the samples $x_i$ from the outer expectation relate to the indices denoted by $i=1,\\dots,N$ from an evaluation dataset $\\mathcal{D}^\\prime$.\n",
    "\n",
    "In the following exercise, please implement the Monte Carlo approximations for the average prediction, the Bias, and the Variance as shown above.\n",
    "\n",
    "Hint: You can and absolutely should use the `predict` function from the regression example as well as the function of the true model `f` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca0c5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_prediction(x, ws, degree):\n",
    "  \"\"\"\n",
    "  Approximation of the average prediction using the M function estimations\n",
    "  Args:\n",
    "    x: Input data of size (N,). N denotes the number of samples in the evaluation dataset.\n",
    "    ws: The weights obtained from ridge regression of size (M, degree). \n",
    "      The first dimension denotes the M different subset datasets on which the estimator is fitted.\n",
    "    degree: The polynomial degree\n",
    "  Returns:\n",
    "    The average prediction as a scalar\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  predictions = []\n",
    "  for w in ws:\n",
    "    pred = predict(x, w, degree)\n",
    "    predictions.append(pred)\n",
    "  return np.stack(predictions).mean(axis=0)\n",
    "  ### END SOLUTION\n",
    "\n",
    "\n",
    "def calc_bias(x_q, ws, degree):\n",
    "  \"\"\"Estimate the bias.\n",
    "  Args:\n",
    "    x_q: Input data of size (N,). N denotes the number of samples in the evaluation dataset.\n",
    "    ws: The weights obtained from ridge regression of size (M, degree)\n",
    "      The first dimension denotes the M different subset datasets on which the estimator is fitted.\n",
    "    degree: The polynomial degree\n",
    "  Returns:\n",
    "    Model Bias as a scalar\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  function_eval = f(x_q)\n",
    "  avg_pred = avg_prediction(x_q, ws, degree)\n",
    "  return np.mean((function_eval - avg_pred) ** 2)\n",
    "  ### END SOLUTION\n",
    "\n",
    "def calc_variance(x_q, ws, degree):\n",
    "  \"\"\"Estimate the model variance\n",
    "  Args:\n",
    "    x_q: Input data of size (N,). N denotes the number of samples in the evaluation dataset.\n",
    "    ws: The weights obtained from ridge regression of size (M, degree)\n",
    "      The first dimension denotes the M different subset datasets on which the estimator is fitted.\n",
    "    degree: The polynomial degree\n",
    "  Returns:\n",
    "    Model variance as a scalar\n",
    "  \"\"\"\n",
    "  ### BEGIN SOLUTION\n",
    "  avg_pred = avg_prediction(x_q, ws, degree)\n",
    "  mean = []\n",
    "  for w in ws:\n",
    "    pred = predict(x_q, w, degree)\n",
    "    mean.append(((pred - avg_pred) ** 2))\n",
    "  return np.mean(np.stack(mean))\n",
    "  ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba6ca1",
   "metadata": {},
   "source": [
    "You can test your implementation by running the below coding snippet. It estimate the bias and variance for $M = 25$ datasets with each dataset containing $N=20$ datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aeea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Settings\n",
    "n = 20\n",
    "m = 25\n",
    "minval = -2.\n",
    "maxval = 2.\n",
    "degree = 9\n",
    "train_datasets = []\n",
    "seed = 3001\n",
    "for i in range(m):\n",
    "  train_datasets.append(generate_data(n_train, minval, maxval, train=True, seed=seed))\n",
    "  seed += 1\n",
    "eval_points = np.linspace(minval, maxval, n)\n",
    "\n",
    "# Estimate the bias and variance\n",
    "biases = []\n",
    "vars = []\n",
    "xs, ys = [], []\n",
    "lambdas = [1e-6, 1e-4, 1e-2, 1e0, 1e2, 1e4, 1e6]\n",
    "# Evaluate the different models, characterized by the regularization parameter lambda\n",
    "for l in lambdas:\n",
    "  w_maps = []\n",
    "  for data in train_datasets:\n",
    "    # Fit the estimator on each individual dataset\n",
    "    w = fit_w(x=data[0], y=data[1], lam=l, degree=degree)\n",
    "    w_maps.append(w)\n",
    "  # Estimate the bias of the model\n",
    "  bias = calc_bias(eval_points, w_maps, degree)\n",
    "  biases.append(bias)\n",
    "  # Estimate the variance of the model\n",
    "  var = calc_variance(eval_points, w_maps, degree)\n",
    "  vars.append(var)\n",
    "  xs.append(np.linspace(minval, maxval, 100))\n",
    "  ys.append(predict(xs[-1], w_maps[0], degree))\n",
    "\n",
    "biases = np.array(biases)\n",
    "vars = np.array(vars)\n",
    "xs = np.stack(xs)\n",
    "ys = np.stack(ys)\n",
    "\n",
    "# Plot the bias and variance for different lambas\n",
    "plt.figure()\n",
    "plt.plot(lambdas, biases, label=\"Bias\")\n",
    "plt.plot(lambdas, vars, label=\"Variance\")\n",
    "plt.plot(lambdas, biases + vars, label=\"Total Err.\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.title(\"Total Error for Different Regularization Values\")\n",
    "\n",
    "# Calculate predictions\n",
    "plot_linear_regression(xs, ys, labels=lambdas, eval_quantity=r\"$\\lambda$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8906acc",
   "metadata": {},
   "source": [
    "## Question 1.5: Interpretation of the bias and variance trade-off\n",
    "Please explain the results in a few sentences. In particular, provide an explanation if the bias and variance behave as expected. For which regularization parameter $\\lambda$ would you decide?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499518b65413e47",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "If we choose very small regularization parameters $\\lambda$, the model tends to overfit to the data, whereas for large $\\lambda$, the model tends to underfit the data. As the polynomial degree is higher than the true function, the model is prone to overfit which can be seen when we do not regularize the model (small $\\lambda$). The overfitting correlates with a high variance as the optimization can get stuck in multiple minima. On the contrary, high regularization restricts the expressive power of the model, and thus yields a high bias.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bcdebb",
   "metadata": {},
   "source": [
    "# Part 2: Clustering using Parametric Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499b341",
   "metadata": {},
   "source": [
    "## Question 2.1: Visualization of the Dataset\n",
    "Before being able to cluster the dataset, you need to obtain an initial impression of the data to cluster. Please generate a scatter plot of the data. What is the optimal number of clusters for the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# Load dataset from .npy file\n",
    "data = np.load('./clustering_dataset.npy')\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Dataset Visualization')\n",
    "plt.show()\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7cccd",
   "metadata": {},
   "source": [
    "Please set the optimal number of clusters based on your empirical observation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "400cb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_NUM_CLUSTERS = (\n",
    "    ### BEGIN SOLUTION\n",
    "    3\n",
    "    ### END SOLUTION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987f989",
   "metadata": {},
   "source": [
    "## Question 2.2: Implementation of the EM Algorithm\n",
    "In the lecture about parametric density estimation, you got introduced to the EM Algorithm to be able to fit a Gaussian Mixture Model (GMM) over your dataset. Please implement the EM algorithm from scratch by filling in the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "224d8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(data, mixture_weights, means, covariances):\n",
    "    \"\"\"\n",
    "    :param data: all data points\n",
    "    :param mixture_weights: mixture weights of Gaussian Mixture Model\n",
    "    :param means: means of Gaussian Mixture Model\n",
    "    :param covariances: covariances of Gaussian Mixture Model\n",
    "\n",
    "    :return: responsibilities of shape (num_data_samples, num_clusters)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    num_data_samples, num_dimensions = data.shape\n",
    "    num_clusters = len(mixture_weights)\n",
    "\n",
    "    # Initialize the responsibility matrix\n",
    "    responsibilities = np.zeros((num_data_samples, num_clusters))\n",
    "\n",
    "    for n in range(num_data_samples):\n",
    "        for k in range(num_clusters):\n",
    "            covariance = covariances[:, :, k]\n",
    "            covariance_det = np.linalg.det(covariance)\n",
    "            diff = data[n] - means[k]\n",
    "            coefficient = 1. / float(((2 * np.pi) ** (float(num_dimensions) / 2)) * np.sqrt(covariance_det))\n",
    "            exponent = np.exp(-0.5 * np.dot(diff.T, np.linalg.lstsq(covariance, diff, rcond=-1)[0]))\n",
    "\n",
    "            responsibilities[n, k] = mixture_weights[k] * coefficient * exponent\n",
    "        responsibilities[n] /= responsibilities[n].sum()\n",
    "\n",
    "    return responsibilities\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e51d3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(data, responsibilities):\n",
    "    \"\"\"\n",
    "    :param data: all data points\n",
    "    :param responsibilities: responsibilities from e_step\n",
    "\n",
    "    :return: updated mixture_weights, updated means, updated covariances\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    num_data_samples, num_dimensions = data.shape\n",
    "    num_clusters = responsibilities.shape[1]\n",
    "\n",
    "    # Create matrices\n",
    "    new_covariances = np.zeros((num_dimensions, num_dimensions, num_clusters))\n",
    "\n",
    "    N_k = responsibilities.sum(axis=0)\n",
    "    new_mixture_weights = N_k / num_data_samples\n",
    "\n",
    "    new_means = np.divide(np.dot(responsibilities.T, data), N_k[:, np.newaxis])\n",
    "\n",
    "    for k in range(num_clusters):\n",
    "        aux_covariance = np.zeros((num_dimensions, num_dimensions))\n",
    "        for n in range(num_data_samples):\n",
    "            diff = data[n] - new_means[k]\n",
    "            aux_covariance = aux_covariance + responsibilities[n, k] * np.outer(diff, diff.T)\n",
    "        new_covariances[:, :, k] = aux_covariance / N_k[k]\n",
    "\n",
    "    return new_mixture_weights, new_means, new_covariances\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2fa78cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, mixture_weights, means, covariances):\n",
    "    \"\"\"\n",
    "    :param data: all data points\n",
    "    :param mixture_weights: mixture weights of Gaussian Mixture Model\n",
    "    :param means: means of Gaussian Mixture Model\n",
    "    :param covariances: covariances of Gaussian Mixture Model\n",
    "\n",
    "    :return: calculated log likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    num_data_samples, num_dimensions = data.shape\n",
    "    num_clusters = len(mixture_weights)\n",
    "\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    for n in range(num_data_samples):\n",
    "        likelihood = 0.0\n",
    "        for k in range(num_clusters):\n",
    "            covariance = covariances[:, :, k]\n",
    "            covariance_det = np.linalg.det(covariance)\n",
    "            diff = data[n] - means[k]\n",
    "            coefficient = 1. / float(((2 * np.pi) ** (float(num_dimensions) / 2)) * np.sqrt(covariance_det))\n",
    "            exponent = np.exp(-0.5 * np.dot(diff.T, np.linalg.lstsq(covariance, diff, rcond=-1)[0]))\n",
    "            likelihood += mixture_weights[k] * coefficient * exponent\n",
    "\n",
    "        log_likelihood += np.log(likelihood)\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b2d18",
   "metadata": {},
   "source": [
    "Now, implement the expectation_maximization function which leverages the previously implemented functions. Given initial values of the mixture weights, means and covariances, you should apply the E- and M-Step until the log-likelihood has converged, i.e. the absolute difference in subsequent values is below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "89a6c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_maximization(data, init_mixture_weights, init_means, init_covariances, threshold):\n",
    "    \"\"\"\n",
    "    :param data: all data points\n",
    "    :param init_mixture_weights: initial mixture weights of Gaussian Mixture Model\n",
    "    :param init_means: initial means of Gaussian Mixture Model\n",
    "    :param init_covariances: initial covariances of Gaussian Mixture Model\n",
    "    :param threshold: threshold for checking convergence\n",
    "\n",
    "    :return: final mixture_weights, final means, final covariances, final responsibilities, final log_likelihood, num_iterations needed till convergence\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    mixture_weights = init_mixture_weights\n",
    "    means = init_means\n",
    "    covariances = init_covariances\n",
    "\n",
    "    # Evaluate initial value of log likelihood\n",
    "    previous_log_likelihood = log_likelihood(data, mixture_weights, means, covariances)\n",
    "\n",
    "    # Iteration counter\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        # E-step: Compute responsibilities\n",
    "        responsibilities = e_step(data, mixture_weights, means, covariances)\n",
    "\n",
    "        # M-step: Update parameters\n",
    "        mixture_weights, means, covariances = m_step(data, responsibilities)\n",
    "\n",
    "        # Compute log likelihood\n",
    "        current_log_likelihood = log_likelihood(data, mixture_weights, means, covariances)\n",
    "\n",
    "        log_likelihood_diff = np.abs(current_log_likelihood - previous_log_likelihood)\n",
    "        print(\"Training Info: Iteration {} Log Likelihood difference  {}\".format(iteration, log_likelihood_diff))\n",
    "\n",
    "        if log_likelihood_diff < threshold:\n",
    "            break\n",
    "\n",
    "        previous_log_likelihood = current_log_likelihood\n",
    "        iteration += 1\n",
    "\n",
    "    return mixture_weights, means, covariances, responsibilities, current_log_likelihood, iteration\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03cc9e",
   "metadata": {},
   "source": [
    "## Question 2.3: Clustering the Dataset and Visualizing the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e9174722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, responsibilities, means, covariances):\n",
    "    \"\"\"\n",
    "    :param data: all data points\n",
    "    :param responsibilities: final responsibilities\n",
    "    :param means: final means\n",
    "    :param covariances: final covariances\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    num_components = len(means)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # label the data points w.r.t to the final responsibilities\n",
    "    labels = np.argmax(responsibilities, axis=1)\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=labels)\n",
    "\n",
    "    # plot the resulting ellipsoids of mixture of Gaussians\n",
    "    for k in range(num_components):\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(covariances[:,:,k])\n",
    "\n",
    "        # Sort eigenvalues and eigenvectors in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[sorted_indices]\n",
    "        eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "        # Calculate the angle of rotation\n",
    "        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "\n",
    "        # Create an ellipse based on the eigenvalues and eigenvectors\n",
    "        ellipse = Ellipse(means[k], 2 * np.sqrt(eigenvalues[0]), 2 * np.sqrt(eigenvalues[1]),\n",
    "                          angle=angle, fill=False, edgecolor='r')\n",
    "\n",
    "        # Plot the ellipse\n",
    "        ax.add_patch(ellipse)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Clustering Result')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5d0f9",
   "metadata": {},
   "source": [
    "Please implement the main function which should load the dataset, execute the EM algorithm and plot the results using the previously implemented functions. Note, the EM algorithm expects initial values for the mixture weights, means and covariances of the Gaussian Mixture Model. Please initialize the mixture weights uniformly over the number of mixture components and the initial covariances with a unit scale. You can experiment around with the initial means as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88092c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./clustering_dataset.npy\"\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_MIXTURE_COMPONENTS = OPTIMAL_NUM_CLUSTERS             # as previously defined by yourself\n",
    "CONVERGENCE_THRESHOLD = 1e-6\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    data = np.load(DATASET_PATH)\n",
    "    num_training_samples, num_dimensions = data.shape\n",
    "\n",
    "    init_mixture_weights = np.ones(NUM_MIXTURE_COMPONENTS) / NUM_MIXTURE_COMPONENTS\n",
    "    init_means = np.array([[2.5, 0], [6.0, 5.0], [7.5, -7.5]])\n",
    "    init_covariances = np.stack([np.eye(num_dimensions)] * NUM_MIXTURE_COMPONENTS, axis=-1)\n",
    "\n",
    "    mixture_weights, means, covariances, responsibilities, final_log_likelihood, iteration = expectation_maximization(data, init_mixture_weights, init_means, init_covariances, CONVERGENCE_THRESHOLD)\n",
    "\n",
    "    print(\"\\n Final weights:\", mixture_weights)\n",
    "    print(\"\\n Final cluster means:\", means)\n",
    "    print(\"\\n Final cluster covariance 1:\", covariances[:, :, 0])\n",
    "    print(\"\\n Final cluster covariance 2:\", covariances[:, :, 1])\n",
    "    print(\"\\n Final cluster covariance 3:\", covariances[:, :, 2])\n",
    "    print(\"\\n Final log likelihood:\", final_log_likelihood)\n",
    "    print(\"\\n EM converged after {} iterations\".format(iteration))\n",
    "\n",
    "    plot_results(data, responsibilities, means, covariances)\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4a89c",
   "metadata": {},
   "source": [
    "## Question 2.4: Report Results\n",
    "Please report the following quantitative information regarding your final solution:\n",
    "- Final Mixture Weights\n",
    "- Initial Means\n",
    "- Final Means\n",
    "- Final Covariances\n",
    "- Final Log Likelihood\n",
    "- Number of Iterations until Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903a7bc",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "See above\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540edb92",
   "metadata": {},
   "source": [
    "# Part 3: Biased estimators\n",
    "For a set of i.i.d. observations $X = \\{x_1, x_2, \\dots, x_n\\}$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ with mean $\\mu$ and variance $\\sigma^2$, we know that the maximum likelihood estimate of the variance is\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2,\n",
    "$$\n",
    "where $\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$ is the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621485c8",
   "metadata": {},
   "source": [
    "## Question 3.1: Bias of the sample variance\n",
    "Please follow the steps below to show that $\\hat{\\sigma}^2$ is a biased estimate of the variance. \n",
    "\n",
    "1. Derive the expected value of $\\hat{\\sigma}^2$, i.e., $\\mathbb{E}_{X}[\\hat{\\sigma}^2]$.\n",
    "2. Show that $\\mathbb{E}_{X}[\\hat{\\sigma}^2] \\neq \\sigma^2$, indicating that the sample variance tends to underestimate the population variance.\n",
    "3. Modify the estimator to obtain an unbiased estimate of the population variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312b62c",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "1. We are tasked with finding the expected value of the sample variance estimator $\\hat{\\sigma}^2$, which is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2\n",
    "$$\n",
    "\n",
    "We can expand this expression as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\mathbb{E} \\left[ \\sum_{i=1}^{N} \\left( x_i^2 - 2 x_i \\overline{x} + \\overline{x}^2 \\right) \\right]\n",
    "$$\n",
    "\n",
    "Now, separate the sums:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\mathbb{E} \\left[ \\sum_{i=1}^{N} x_i^2 - 2 \\sum_{i=1}^{N} x_i \\overline{x} + N \\overline{x}^2 \\right]\n",
    "$$\n",
    "\n",
    "We can simplify this further by noting the following identities:\n",
    "\n",
    "- $\\sum_{i=1}^{N} x_i = N \\overline{x}$ (since $\\overline{x}$ is the sample mean),\n",
    "- $\\sum_{i=1}^{N} \\overline{x}^2 = N \\overline{x}^2$ (since $\\overline{x}$ is constant).\n",
    "\n",
    "Substitute these into the expression:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\mathbb{E} \\left[ \\sum_{i=1}^{N} x_i^2 - 2N \\overline{x}^2 + N \\overline{x}^2 \\right]\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\mathbb{E} \\left[ \\sum_{i=1}^{N} x_i^2 - N \\overline{x}^2 \\right]\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\mathbb{E} \\left[ \\sum_{i=1}^{N} x_i^2 \\right] - \\mathbb{E} \\left[ \\overline{x}^2 \\right]\n",
    "$$\n",
    "\n",
    "We know that:\n",
    "\n",
    "- $\\mathbb{E}[x_i^2] = \\sigma_x^2 + \\mu^2$ for each $x_i$, where $\\mu$ is the population mean.\n",
    "- $\\mathbb{E}[\\overline{x}] = \\mu$.\n",
    "\n",
    "Therefore, the first term becomes:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\mathbb{E} \\left[ \\sum_{i=1}^{N} x_i^2 \\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sigma_x^2 + \\mu^2 \\right) = \\sigma_x^2 + \\mu^2\n",
    "$$\n",
    "\n",
    "For the second term, we calculate $\\mathbb{E}[\\overline{x}^2]$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\overline{x}^2] = \\text{Var}[\\overline{x}] + \\mathbb{E}[\\overline{x}]^2\n",
    "$$\n",
    "\n",
    "Since $\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$, the variance of $\\overline{x}$ is:\n",
    "\n",
    "$$\n",
    "\\text{Var}[\\overline{x}] = \\frac{1}{N^2} \\text{Var} \\left[ \\sum_{i=1}^{N} x_i \\right] = \\frac{1}{N} \\sigma_x^2\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\overline{x}^2] = \\frac{\\sigma_x^2}{N} + \\mu^2\n",
    "$$\n",
    "\n",
    "Substituting these results back, we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\sigma_x^2 + \\mu^2 - \\left( \\frac{\\sigma_x^2}{N} + \\mu^2 \\right)\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\sigma_x^2 \\left( 1 - \\frac{1}{N} \\right)\n",
    "$$\n",
    "\n",
    "Thus, the expected value of the sample variance is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{N-1}{N} \\sigma_x^2\n",
    "$$\n",
    "\n",
    "2. This shows that $\\mathbb{E}[\\hat{\\sigma}^2]$ is slightly smaller than $\\sigma_x^2$, meaning the sample variance tends to underestimate the true population variance.\n",
    "\n",
    "3. To correct for this bias, we modify the estimator to:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_{\\text{unbiased}} = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\overline{x})^2\n",
    "$$\n",
    "\n",
    "This modification ensures that the expectation of the sample variance is exactly equal to the population variance, $\\sigma_x^2$.\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353b252",
   "metadata": {},
   "source": [
    "# Part 4: Classification\n",
    "\n",
    "The lecture introduces logistic regression as a solution to a binary classification problem in which the class posterior is modeled by a linear discriminant model and a sigmoid activation function $p(C_1 | \\bm{x}) = \\sigma(\\bm{w}^\\intercal \\bm{x} + b)$. Here $w$ denotes the weight of the linear function and $b$ the shift in y-direction. Instead of following classical maximum likelihood ideas to estimate the parameters $w$ and $b$, we want to implement a very simple, yet powerful, binary classification algorithm - The perceptron algorithm. Instead of the sigmoid function, we instead use the sign function\n",
    "$$\n",
    "\\mathrm{sign}(\\bm{x}) =\\begin{cases}\n",
    "    1, \\quad \\mathrm{if}~\\bm{x}>0, \\\\\n",
    "    0, \\quad \\mathrm{if}~\\bm{x}=0, \\\\\n",
    "    -1, \\quad \\mathrm{if}~\\bm{x}<0.\n",
    "\\end{cases}\n",
    "$$\n",
    "to directly estimate the class using the discriminant function $y(x) = \\mathrm{sign}(\\bm{w}^\\intercal \\bm{x} + b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079980ff",
   "metadata": {},
   "source": [
    "To introduce the problem, we First generate some synthetic data, where each datapoint belongs to on of two classes (-1 or 1). We further highlight the optimal discriminant function in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e972a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification(data, w, b, classes):\n",
    "    \"\"\"Helper function for plotting the data and decision boundary\"\"\"\n",
    "\n",
    "    x_min, x_max = min(data[:, 0]), max(data[:, 0])\n",
    "    y_min, y_max = min(data[:, 1]), max(data[:, 1])\n",
    "\n",
    "    x = np.linspace(x_min, x_max)\n",
    "    if w[1] == 0:\n",
    "        plt.plot(x, np.ones_like(x) * -b, color=\"green\", label=\"decision boundary\")\n",
    "    else:\n",
    "        plt.plot(x, (-b -w[0] * x) / w[1], color=\"green\", label=\"decision boundary\")\n",
    "    plt.scatter(data[classes, 0], data[classes, 1], color=\"blue\", label=\"Class 1\")\n",
    "    plt.scatter(data[~classes, 0], data[~classes, 1], color=\"red\", label=\"Class 2\")\n",
    "    plt.xlim((x_min, x_max))\n",
    "    plt.ylim((y_min, y_max))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (2 * np.random.random(size=(50, 2)) - 1) * 10\n",
    "w_theoric = np.array([1, 2])\n",
    "b_theoric = 2\n",
    "\n",
    "classes = (data @ w_theoric.T + b_theoric) > 0\n",
    "labels = 2 * classes.astype(int) - 1\n",
    "\n",
    "plot_classification(data, w_theoric, b_theoric, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bb9bb",
   "metadata": {},
   "source": [
    "## Question 4.1: The perceptron algorithm\n",
    "\n",
    "The perceptron algorithm is a simple algorithm to estimate the weights $w$ and $b$ for the perceptron discriminant function $y(\\bm{x}) = \\mathrm{sign}(\\bm{w}^\\intercal \\bm{x} + b)$.\n",
    "\n",
    "Above we plot the data as well as the ground truth linear decision boundary which was used to generate the data.Naturally the question arises \"If we don't know the ground truth paramethers, how can we find them?\"\n",
    "For this, you should implement the perceptron algorithm within the next function.\n",
    "\n",
    "### Perceptron algorithm\n",
    "- Inititalize the weight $w$ and bias $b$\n",
    "- For all pairs of data points $(\\bm{x}_i, y_i)$, repeat until convergence\n",
    "    - If $x_i$ is correctly classified, i.e., $y(\\bm{x}_i) = y_i$, do nothing,\n",
    "    - else if $y_i = 1$ update the parameters with $\\bm{w} \\leftarrow \\bm{w} + \\gamma \\bm{x}_i, b \\leftarrow b + 1$,\n",
    "    - else if $y_i = -1$ update the parameters with $\\bm{w} \\leftarrow \\bm{w} - \\gamma \\bm{x}_i, b \\leftarrow b - 1$\n",
    "\n",
    "Here, $\\gamma$ ia a learning rate which balances how much the weight is moved towards the sample $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57599c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(data, labels, iterations=100, learning_rate=0.1):\n",
    "    \n",
    "    w = np.zeros(2)\n",
    "    b = 0\n",
    "\n",
    "    print('Initial')\n",
    "    plot_classification(data, w, b, classes)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        random_idx = np.random.randint(0, len(data))\n",
    "        x, y = data[random_idx], labels[random_idx]\n",
    "        pred = x @ w + b\n",
    "        \n",
    "        if y == 1 and pred <= 0:\n",
    "            w += learning_rate * x\n",
    "            b += 1\n",
    "        elif y == -1 and pred >= 0:\n",
    "            w -= learning_rate * x\n",
    "            b -= 1\n",
    "\n",
    "        ### END SOLUTION\n",
    "\n",
    "    print('Final solution')\n",
    "    plot_classification(data, w, b, classes)\n",
    "    return w, b\n",
    "\n",
    "w,b = perceptron(data, labels, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839b52d",
   "metadata": {},
   "source": [
    "### Question 4.2: Intuition on the update steps\n",
    "Explain intuitively how the update steps in the perceptron algorithm are similar to gradient descent, and why they help the model improve its classification decision over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c9a97",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "The updates rotate and shift the hyperplane spaned by $\\bm{w}$ and $b$ in the case of mis-classification. \n",
    "\n",
    "The updates can be thought of as the gradients, $\\nabla_{\\bm{w}}\\mathcal{L}(\\bm{w}, b)$ and $\\nabla_{b}\\mathcal{L}(\\bm{w}, b)$, of the classification loss\n",
    "$$\\mathcal{L}(\\bm{w}, b) = \\max(0, -y_i (\\bm{w}^\\intercal \\bm{x}_i + b)).$$\n",
    "Thus, the perceptron algorithm can be seen as a first order gradient descent optimization method.\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}